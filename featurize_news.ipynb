{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1m9v35TGO0OOMzlT4exu7ahfYXXRampiz",
      "authorship_tag": "ABX9TyN6lQ4+T51q6lTl/eqRGpmL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcolinmorgan/mlx_grph/blob/main/featurize_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNPa1N_VETtt",
        "outputId": "92df6cec-28f7-435f-c6ff-fbfc7956e6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "## adapted from https://jaketae.github.io/study/keyword-extraction/#candidate-selection\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"distilroberta-base\"\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "n_gram_range = (1, 2)\n",
        "stop_words = \"english\"\n",
        "embeddings=[]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "import nltk, string, numpy as np\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4miUEh-WD7G",
        "outputId": "2b372aad-2de5-42ec-d002-6d998b3d5be6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_npr_stories(p):\n",
        "    # Send a GET request to the NPR API\n",
        "    r = requests.get(\"http://api.npr.org/query?apiKey=MDE5Mzg3Mjc2MDE0MzMyMjM3NjM5ZTI2Ng001\", params=p)\n",
        "\n",
        "    # Parse the XML response to get the story URLs\n",
        "    root = ET.fromstring(r.content)\n",
        "    story_urls = [story.find('link').text for story in root.iter('story')]\n",
        "\n",
        "    # For each story URL, send a GET request to get the HTML content\n",
        "    full_stories = []\n",
        "    for url in story_urls:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the main content of the story. This will depend on the structure of the webpage.\n",
        "        # Here, we're assuming that the main content is in a <p> tag. You might need to adjust this depending on the webpage structure.\n",
        "        story = soup.find_all('p')\n",
        "\n",
        "        # Extract the text from the story\n",
        "        full_story = ' '.join(p.text for p in story)\n",
        "        full_stories.append(full_story)\n",
        "    return full_stories\n"
      ],
      "metadata": {
        "id": "RJjNm4qvEZoD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_len):\n",
        "    # Tokenize the text into tokens\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Calculate the number of chunks and the size of the final chunk\n",
        "    num_chunks = len(tokens) // max_len\n",
        "    final_chunk_size = len(tokens) % max_len\n",
        "\n",
        "    # If the final chunk is too small, distribute its tokens among the other chunks\n",
        "    if final_chunk_size < max_len / 2:\n",
        "        num_chunks += 1\n",
        "        chunk_sizes = [len(tokens) // num_chunks + (1 if i < len(tokens) % num_chunks else 0) for i in range(num_chunks)]\n",
        "        chunks = [tokens[sum(chunk_sizes[:i]):sum(chunk_sizes[:i+1])] for i in range(num_chunks)]\n",
        "    else:\n",
        "        chunks = [tokens[i:i + max_len] for i in range(0, len(tokens), max_len)]\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "EnFhQIkrEZs5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def featurize_stories(text, max_len, top_k):\n",
        "    # Extract candidate words/phrases\n",
        "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
        "    all_candidates = count.get_feature_names_out()\n",
        "    doc = nlp(text)\n",
        "    noun_phrases = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
        "    nouns = set()\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            nouns.add(token.text)\n",
        "\n",
        "    all_nouns = nouns.union(noun_phrases)\n",
        "    candidates = list(filter(lambda candidate: candidate in all_nouns, all_candidates))\n",
        "    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=\"pt\")\n",
        "    candidate_tokens = {k: v.to(device) for k, v in candidate_tokens.items()}\n",
        "    candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
        "    candidate_embeddings = candidate_embeddings.detach()#.to_numpy()\n",
        "\n",
        "    # words = nltk.word_tokenize(text)\n",
        "    # chunks = [words[i:i + 512] for i in range(0, len(words), 512)]\n",
        "    chunks = chunk_text(text, max_len)  # use this to chunk better and use less padding thus less memory but also less affect from averging\n",
        "\n",
        "    for chunk in chunks:\n",
        "        text_tokens = tokenizer(chunk, padding=True, return_tensors=\"pt\")\n",
        "        text_tokens = {k: v.to(device) for k, v in text_tokens.items()}\n",
        "        text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
        "        text_embedding = text_embedding.detach()#.to_numpy()\n",
        "        embeddings.append(text_embedding)\n",
        "    max_emb_shape = max(embedding.shape[0] for embedding in embeddings)\n",
        "    padded_embeddings = [np.pad(embedding.cpu(), ((0, max_emb_shape - embedding.shape[0]), (0, 0))) for embedding in embeddings]\n",
        "    avg_embedding = np.min(padded_embeddings, axis=0)\n",
        "    distances = cosine_similarity(avg_embedding, candidate_embeddings.cpu())\n",
        "\n",
        "    return [candidates[index] for index in distances.argsort()[0][::-1][-top_k:]]\n",
        "\n"
      ],
      "metadata": {
        "id": "RR_2MZ5nEZvO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# data=pd.read_csv('/content/drive/MyDrive/consult/Louie_disaster_tweets.csv',header=None)\n",
        "data=pd.read_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet.csv')"
      ],
      "metadata": {
        "id": "3mlI9cOaEsnS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaqzAonQGa11",
        "outputId": "2d189558-e7d7-498a-f0ff-2a955ffefaf0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       #DoNothingDeSantis\\nWith Hurricane Season Loom...\n",
              "1       @drscribblesmd I'll be so glad when hurricane ...\n",
              "2       @wideawake_media @useful_eater The Florida hur...\n",
              "3       @135knots The hurricane landfall better be awa...\n",
              "4                            @WSJ It’s Hurricane season. \n",
              "                              ...                        \n",
              "8881    @Shiri8580 @Chandrakbose Ur story of Savarkar ...\n",
              "8882    Accuracy of labeling refers to the ingredients...\n",
              "8883    @citizentvkenya Is she even still a Bishop? An...\n",
              "8884    🔥 New RetroDrop : DistricOne x OpenLaverage \\n...\n",
              "8885    @alikous I think what makes him more willing i...\n",
              "Name: text, Length: 8886, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features.txt',sep='\\t')"
      ],
      "metadata": {
        "id": "e5VsbTP7WaSH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_articles=[]\n",
        "from tqdm import tqdm\n",
        "for i in tqdm(range(len(df),len(data['text']))):\n",
        "    try:\n",
        "        cc=featurize_stories(data['text'][i], max_len=512, top_k=4)\n",
        "        # print(cc)\n",
        "        rank_articles.append(cc)\n",
        "    except IndexError:\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta3pWa5BEZx2",
        "outputId": "05954fcf-38d7-49e8-c740-60ab27675e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 788/5515 [01:52<18:18,  4.30it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_list = [item for sublist in rank_articles for item in sublist]\n",
        "from collections import Counter\n",
        "counter = Counter(flattened_list)\n",
        "df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n",
        "\n",
        "df = df.sort_values(by='Count',ascending=False)\n",
        "df.to_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features.txt',sep='\\t')"
      ],
      "metadata": {
        "id": "Q3pN0rHtfWon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8_w0zPwWX2T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}