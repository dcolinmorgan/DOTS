{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run model on sample of OpenSearch tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "def is_installed(package):\n",
    "    try:\n",
    "        pkg_resources.get_distribution(package)\n",
    "        return True\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False\n",
    "\n",
    "print(is_installed('cudf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "# import concurrent.futures\n",
    "# from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch, spacy,nltk,subprocess, json, requests,string,csv\n",
    "\n",
    "model_name = \"distilroberta-base\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "n_gram_range = (1, 2)\n",
    "stop_words = \"english\"\n",
    "embeddings=[]\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "import nltk, string, numpy as np\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npr_news(p):\n",
    "    # Send a GET request to the NPR API\n",
    "    r = requests.get(\"http://api.npr.org/query?apiKey=npr_key\", params=p)\n",
    "\n",
    "    # Parse the XML response to get the story URLs\n",
    "    root = ET.fromstring(r.content)\n",
    "    story_urls = [story.find('link').text for story in root.iter('story')]\n",
    "\n",
    "    # For each story URL, send a GET request to get the HTML content\n",
    "    full_stories = []\n",
    "    for url in story_urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content of the story. This will depend on the structure of the webpage.\n",
    "        # Here, we're assuming that the main content is in a <p> tag. You might need to adjust this depending on the webpage structure.\n",
    "        story = soup.find_all('p')\n",
    "\n",
    "        # Extract the text from the story\n",
    "        full_story = ' '.join(p.text for p in story)\n",
    "        full_stories.append(full_story)\n",
    "    return full_stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_len):\n",
    "    # Tokenize the text into tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Calculate the number of chunks and the size of the final chunk\n",
    "    num_chunks = len(tokens) // max_len\n",
    "    final_chunk_size = len(tokens) % max_len\n",
    "\n",
    "    # If the final chunk is too small, distribute its tokens among the other chunks\n",
    "    if final_chunk_size < max_len / 2:\n",
    "        num_chunks += 1\n",
    "        chunk_sizes = [len(tokens) // num_chunks + (1 if i < len(tokens) % num_chunks else 0) for i in range(num_chunks)]\n",
    "        chunks = [tokens[sum(chunk_sizes[:i]):sum(chunk_sizes[:i+1])] for i in range(num_chunks)]\n",
    "    else:\n",
    "        chunks = [tokens[i:i + max_len] for i in range(0, len(tokens), max_len)]\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def featurize_stories(text, max_len, top_k):\n",
    "    # Extract candidate words/phrases\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "    all_candidates = count.get_feature_names_out()\n",
    "    doc = nlp(text)\n",
    "    noun_phrases = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
    "    nouns = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.add(token.text)\n",
    "\n",
    "    all_nouns = nouns.union(noun_phrases)\n",
    "    candidates = list(filter(lambda candidate: candidate in all_nouns, all_candidates))\n",
    "    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=\"pt\")\n",
    "    # candidate_tokens = {k: v.to(device) for k, v in (candidate_tokens).items()}\n",
    "    candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
    "    candidate_embeddings = candidate_embeddings.detach()#.to_numpy()\n",
    "\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    # chunks = [words[i:i + 512] for i in range(0, len(words), 512)]\n",
    "    chunks = chunk_text(text, max_len)  # use this to chunk better and use less padding thus less memory but also less affect from averging\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text_tokens = tokenizer(chunk, padding=True, return_tensors=\"pt\")\n",
    "        text_tokens = {k: v.to(device) for k, v in (text_tokens).items()}\n",
    "        text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
    "        text_embedding = text_embedding.detach()#.to_numpy()\n",
    "        embeddings.append(text_embedding)\n",
    "    max_emb_shape = max(embedding.shape[0] for embedding in embeddings)\n",
    "    padded_embeddings = [np.pad(embedding.cpu(), ((0, max_emb_shape - embedding.shape[0]), (0, 0))) for embedding in embeddings]\n",
    "    avg_embedding = np.min(padded_embeddings, axis=0)\n",
    "    distances = cosine_similarity(avg_embedding, candidate_embeddings.cpu())\n",
    "    torch.cuda.empty_cache()\n",
    "    return [candidates[index] for index in distances.argsort()[0][::-1][-top_k:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# data=pd.read_csv('/content/drive/MyDrive/consult/Louie_disaster_tweets.csv',header=None)\n",
    "data=pd.read_csv('florida-hurricane-tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('8.5k_florida_tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_articles=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(df),len(data['text']))):\n",
    "    try:\n",
    "        cc=featurize_stories(data['text'][i], max_len=512, top_k=4)\n",
    "        # print(cc)\n",
    "        rank_articles.append(cc)\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in rank_articles for item in sublist]\n",
    "from collections import Counter\n",
    "counter = Counter(flattened_list)\n",
    "df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n",
    "\n",
    "df = df.sort_values(by='Count',ascending=False)\n",
    "df.to_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "print(len(df))\n",
    "# df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [sublist for sublist in rank_articles if any('fire'.lower() in s.lower() for s in sublist)]\n",
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('florida-hurricane-tweet_features.txt',sep='\\t')\n",
    "df2=pd.read_csv('florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "df=pd.concat([df,df2])\n",
    "print(df['Unnamed: 0'])\n",
    "df = df.groupby('Unnamed: 0').sum().sort_values(by='Count',ascending=False)\n",
    "df=df[df['Count']>int(np.round(len(df)*.001))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# nouns = ['apple', 'John', 'London', 'dog', 'Mary', 'Paris', 'banana']\n",
    "nouns= df.reset_index()['Unnamed: 0'].to_list()\n",
    "doc = nlp(' '.join(nouns))\n",
    "\n",
    "proper_nouns = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "print(proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(proper_nouns))\n",
    "proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=['Iceland', '16-01-2024', ['Lava from a volcanic eruption in southwestern Iceland has streamed into a nearby town, engulfing homes and forcing the evacuation of local residents.\\n', 'The Icelandic Meteorological Office said the eruption began just before 8:00 a.m. local time Sunday about a half mile from the town of Grindavík after a series of intense earthquakes.\\n', 'A second fissure opened after noon Sunday and sent lava flows into the town, officials said.\\n', 'Iceland\\'s president, Guðni Th. Jóhannesson, said in an address to the nation on Sunday that a \"daunting period of upheaval\" had begun for those in the Reykjanes peninsula.\\n', '\"We continue to hope for as good an outcome as possible, in the face of these tremendous forces of nature,\" he said.\\n', 'This is the second time in a month that a volcano has erupted just outside Grindavík, a coastal town about 25 miles from the Icelandic capital of Reykjavík.\\n', 'An eruption on December 18 sent lava spewing into the air, but residents had already been told to leave due to heightened seismic activity.\\n', \"Iceland's government said Sunday that Grindavík had been evacuated early that morning and that the eruption isn't expected to reach other populated areas.\\n\", 'So far no flights have been disrupted, and officials are monitoring threats to infrastructure. At least three homes have either burned down or been overtaken by lava, according to the Icelandic broadcaster RUV.\\n', \"The government also said the eruption doesn't present a threat to life.\\n\", 'But in his speech, Jóhannesson offered his sympathy to the loved ones of Lúðvík Pétursson, a man who went missing in a work accident in Grindavík last week.\\n', \"According to Sky News, the 50-year-old Pétursson was filling crevasses formed by volcanic activity and earthquakes when he fell in a crack that had opened after last month's eruption.\\n\", 'Iceland is a hotspot for seismic activity, with 32 active volcanoes. A volcano erupts roughly every five years in the country, though eruptions have occurred more frequently recently. \\n', ' Copyright 2024 NPR. To see more, visit https://www.npr.org.  ', '\\n'], None, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameterize functions\n",
    " - parallelize text pulls from urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, signal,datetime,subprocess,json,os\n",
    "from dotenv import load_dotenv\n",
    "parser = argparse.ArgumentParser(description='Process OS data for dynamic features.')\n",
    "parser.add_argument('-n', type=int, default=10, help='Number of data items to get')\n",
    "parser.add_argument('-f', type=int, default=3, help='Number of features per item to get')\n",
    "parser.add_argument('-o', type=str, default='dots_feats.csv', help='Output file name')\n",
    "parser.add_argument('-p', type=int, default=1, help='Parallelize requests')\n",
    "# parser.add_argument('-s', type=datetime, default=20230101, help='start date')\n",
    "# parser.add_argument('-e', type=datetime, default=20231231, help='end date')\n",
    "args, unknown = parser.parse_known_args()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import get_OS_data\n",
    "dd=get_OS_data(10)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os_url=([os.environ['OS_TOKEN']])\n",
    "\n",
    "os_url = os.getenv('OS_TOKEN')\n",
    "n=10\n",
    "bash_command = f\"\"\"\n",
    "curl -X GET \"{os_url}/emergency-management-news/_search?scroll=1m\" -H 'Content-Type: application/json' -d '{{\n",
    "\"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "    \"size\": {n},\n",
    "    \"query\": {{\n",
    "        \"bool\": {{\n",
    "            \"must\": [\n",
    "                {{\"match_all\": {{}}}}\n",
    "            ]\n",
    "        }}\n",
    "    }}\n",
    "}}'\n",
    "\"\"\"\n",
    "process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "output = process.stdout\n",
    "data = json.loads(output)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.loads(\"input/feat_input.json\")\n",
    "with open(\"input/feat_input.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "data['_scroll_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the scroll ID\n",
    "scroll_id = response['_scroll_id']\n",
    "es = OpenSearch([os.environ['OS_TOKEN']])\n",
    "# Keep scrolling until no more results\n",
    "while len(response['hits']['hits']):\n",
    "    # print(response['hits']['hits'])  # process results\n",
    "\n",
    "    response = es.scroll(\n",
    "        scroll_id=scroll_id,\n",
    "        scroll='1m'  # length of time to keep the scroll window open\n",
    "    )\n",
    "\n",
    "    # Update the scroll ID\n",
    "    scroll_id = response['_scroll_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import featurize_stories, process_data, get_OS_data, process_hit\n",
    "data = get_OS_data(10)\n",
    "data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,argparse,datetime\n",
    "sys.argv = ['dots_feat.py', '-n', '10', '-f', '3', '-o', 'dots_feats.csv', '-p', '1']#, '-s', '20230101', '-e', '20231231']\n",
    "parser = argparse.ArgumentParser(description='Process OS data for dynamic features.')\n",
    "parser.add_argument('-n', type=int, default=10, help='Number of data items to get')\n",
    "parser.add_argument('-f', type=int, default=3, help='Number of features per item to get')\n",
    "parser.add_argument('-o', type=str, default='dots_feats.csv', help='Output file name')\n",
    "parser.add_argument('-p', type=int, default=1, help='Parallelize requests')\n",
    "# parser.add_argument('-s', type=datetime.date, default=datetime.datetime.strptime(20230101), help='start date')\n",
    "# parser.add_argument('-e', type=datetime.date, default=datetime.datetime.strptime(20231231), help='end date')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "from dots.dots_feat import featurize_stories, process_data, get_OS_data, process_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"20230101\"\n",
    "e=\"20231231\"\n",
    "\n",
    "s_date = datetime.datetime.strptime(s, \"%Y%m%d\")\n",
    "e_date = datetime.datetime.strptime(e, \"%Y%m%d\")\n",
    "\n",
    "# Convert datetime objects to ISO 8601 format\n",
    "s_iso = s_date.isoformat() + \"Z\"\n",
    "e_iso = e_date.isoformat() + \"Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s_iso,e_iso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OS_data(n=args.n):\n",
    "    bash_command = f\"\"\"\n",
    "    curl -X GET \"{os_url}\" -H 'Content-Type: application/json' -d '{{\n",
    "\"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "        \"size\": {n},\n",
    "        \"query\": {{\n",
    "            \"bool\": {{\n",
    "                \"must\": [\n",
    "                    {{\"match_all\": {{}}}},\n",
    "                ]\n",
    "            }}\n",
    "        }}\n",
    "    }}'\n",
    "    \"\"\"\n",
    "    process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "    output = process.stdout\n",
    "    data = json.loads(output)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "hits = data['hits']['hits']\n",
    "hit = hits[1]\n",
    "hit['_source']\n",
    "\n",
    "from datetime import datetime\n",
    "source = hit['_source']\n",
    "date = datetime.strptime(source['metadata']['GDELT_DATE'], \"%Y%m%d%H%M%S\")\n",
    "loc = source['metadata']['Locations']\n",
    "# ex = None #source['metadata']['Extras']\n",
    "title = source['metadata']['page_title']\n",
    "url = source['metadata']['DocumentIdentifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = source['metadata']['Locations']\n",
    "\n",
    "s = s.replace(\"'\", '\"')  # json requires double quotes for keys and string values\n",
    "list_of_dicts = json.loads(s)\n",
    "\n",
    "\n",
    "# location_full_name = source['metadata']['Locations']['Location FullName']\n",
    "\n",
    "for dict in list_of_dicts:\n",
    "    if 'Location FullName' in dict:\n",
    "        print(dict['Location FullName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for text, soup,date,loc,title in results:\n",
    "            articles.append([loc,loc,loc,date,title,title,text])\n",
    "            # articles.append(text)\n",
    "\n",
    "            writer.writerow([loc,loc,loc,date,title,title,text]) # force location into top feature, also assume title has important info\n",
    "            # writer.writerow(text)\n",
    "            writer.writerow(['\\n'])\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import process_data_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hits(hit):\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get {url}\")\n",
    "        return z\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all(['p','div','a'])\n",
    "    if not paragraphs:\n",
    "        print(f\"No <p> tags in {url}\")\n",
    "        return z\n",
    "\n",
    "    for p in paragraphs:\n",
    "        z.append(p.get_text())\n",
    "    return z\n",
    "\n",
    "with open('output.html', 'w') as file:\n",
    "    file.write(str(soup))\n",
    "    \n",
    "with open('output.csv', 'w') as file:\n",
    "    file.write(str(z))\n",
    "\n",
    "# return articles\n",
    "articles=[]\n",
    "hits = data['hits']['hits']\n",
    "for hit in hits:\n",
    "    z=process_hits(hit)\n",
    "    articles.append(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_hit(hit):\n",
    "    z = []\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    location = source['metadata'][\"Locations\"]\n",
    "    extras = source['metadata'][\"Extras\"]\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for p in soup.find_all('p'):\n",
    "        z.append(p.get_text())\n",
    "    z.append(page_title)\n",
    "    return z, soup\n",
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for z, soup in results:\n",
    "            writer.writerow(z)\n",
    "            writer.writerow(['\\n'])\n",
    "            articles.append(z)\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "# articles = process_data(data)\n",
    "hits = data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "results=[]\n",
    "e = concurrent.futures.ThreadPoolExecutor()\n",
    "    # try:\n",
    "future = e.submit(process_hit, hits[1])\n",
    "result = future.result(timeout=5)  # Set timeout for 5 seconds\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hits']['hits'][1]['_source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests,os,csv,logging\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(response):\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    output=[]\n",
    "    for iii,hit in enumerate(hits):\n",
    "\n",
    "        source = hit[\"_source\"]\n",
    "        text=[]\n",
    "        try:\n",
    "            date = datetime.strptime(source['metadata']['GDELT_DATE'], \"%Y%m%d%H%M%S\")\n",
    "            date = formatted_date = date.strftime(\"%d-%m-%Y\")\n",
    "            loc = source['metadata']['Locations']\n",
    "            loc = loc.replace(\"'\", '\"')  # json requires double quotes for keys and string values\n",
    "            try:\n",
    "                list_of_dicts = json.loads(loc)\n",
    "                location_full_names = [dict['Location FullName'] for dict in list_of_dicts if 'Location FullName' in dict]\n",
    "                loc = location_full_names[0]\n",
    "            except:\n",
    "                loc = None\n",
    "            org = source['metadata']['Organizations']\n",
    "            per = source['metadata']['Persons']\n",
    "            theme = source['metadata']['Themes'].rsplit('_')[-1]\n",
    "            title = source['metadata']['page_title']\n",
    "            url = source['metadata']['DocumentIdentifier']\n",
    "            output.append([date, loc, title, org, per, theme, url])\n",
    "            # pagination_id=response['_scroll_id']\n",
    "        \n",
    "        except:\n",
    "            # pagination_id=None\n",
    "            pass\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, client = get_massive_OS_data(args.n,args.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response['hits']['hits']\n",
    "# response[\"_scroll_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=[]\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "while len(hits) != 0:\n",
    "    # try:\n",
    "    response = client.scroll(\n",
    "        scroll='5m',\n",
    "        scroll_id=pagination_id\n",
    "            )\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    article = process_response(response)\n",
    "    articles.append(article)\n",
    "    # except:\n",
    "    #     print(\"A ConnectionTimeout error occurred.\")\n",
    "    #     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flattened_list = [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scroll OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()\n",
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scrape import get_OS_data, get_massive_OS_data,get_google_news\n",
    "from pull import process_hit, process_data,pull_data\n",
    "os.getcwd()\n",
    "# os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scroll_data():\n",
    "    response, client = get_massive_OS_data(1)\n",
    "    pagination_id = response[\"_scroll_id\"]\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    articles=[]\n",
    "    articles2=[]\n",
    "    while len(hits) != 0 and len(articles2) < 11000:\n",
    "        response = client.scroll(\n",
    "            scroll='1m',\n",
    "            scroll_id=pagination_id\n",
    "                )\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        # article = process_data(response)\n",
    "        articles.append(hits)\n",
    "        articles2 = [item for sublist in articles for item in sublist]\n",
    "    return [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(location_str):\n",
    "    if location_str:\n",
    "        try:\n",
    "            location_list = json.loads(location_str.replace(\"'\", '\"'))\n",
    "            return [dict['Location FullName'] for dict in location_list if 'Location FullName' in dict]\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all(['p'])\n",
    "    text = []\n",
    "    for p in paragraphs:\n",
    "        text.append(p.get_text())\n",
    "    return text\n",
    "\n",
    "import json\n",
    "import concurrent.futures, requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [list(d['_source']['metadata'].values()) for d in articles]\n",
    "df = pd.DataFrame(data, columns=['date','title', 'person', 'org', 'location', 'theme', 'text', 'url'])\n",
    "df.date=pd.to_datetime(df.date).dt.strftime('%d-%m-%Y')\n",
    "df['locc'] = df['location'].apply(extract_location)\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    df['text'] = list(executor.map(process_url, df['url']))\n",
    "return df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## small gnews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "from tqdm import tqdm\n",
    "from scrape import get_OS_data, get_massive_OS_data,get_google_news\n",
    "from pull import process_hit, process_data,pull_data\n",
    "# from feat import featurize_stories\n",
    "google_news = GNews()\n",
    "\n",
    "# google_news.period = '7d'  # News from last 7 days\n",
    "google_news.max_results = 10000  # number of responses across a keyword\n",
    "google_news.country = 'United States'  # News from a specific country \n",
    "google_news.language = 'english'  # News in a specific language\n",
    "google_news.exclude_websites = ['yahoo.com', 'cnn.com']  # Exclude news from specific website i.e Yahoo.com and CNN.com\n",
    "google_news.start_date = (2024, 1, 1) # Search from 1st Jan 2020\n",
    "google_news.end_date = (2024, 3, 1) # Search until 1st March 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_resp = google_news.get_news('disaster')\n",
    "article=[]\n",
    "\n",
    "for i in tqdm(range(len(json_resp)), desc=\"grabbing directly from GoogleNews\"):\n",
    "    aa=(google_news.get_full_article(json_resp[i]['url']))\n",
    "    try:\n",
    "        date=aa.publish_date.strftime(\"%d-%m-%Y\")\n",
    "    except:\n",
    "        date=None\n",
    "    try:\n",
    "        title=aa.title\n",
    "        text=aa.text\n",
    "    except:\n",
    "        title=None\n",
    "        text=None\n",
    "    article.append([title,date,text])\n",
    "\n",
    "# return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(article, desc=\"featurizing articles\"):\n",
    "    foreparts=str(i).split(',')[:2]  # location and date\n",
    "    meat=\"\".join(str(i).split(',')[2:-3])  # article text\n",
    "    cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lobstr API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "lobstr_key = os.environ['LOBSTR_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA['id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 'https://api.lobstr.io/v1/runs/{AA['id'][0]}/stats' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!curl 'https://api.lobstr.io/v1/runs?page=1&page_size=3000' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\\n",
    "    -o 'input/runs.json'\n",
    "    \n",
    "with open(\"input/runs.json\", 'r') as f:\n",
    "    runs = json.load(f)\n",
    "juns=pd.DataFrame(runs['data'])\n",
    "AA=juns[['id','cluster','total_unique_results']]\n",
    "latest_success_run = AA.loc[AA['total_unique_results'].ne(0).idxmax()]\n",
    "\n",
    "!curl 'https://api.lobstr.io/v1/results?cluster=8de6e1bbf33f47b8bce451075b883252&run={latest_success_run['id']}&page=1&page_size=3000' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\\n",
    "    -o 'input/lobstr_results.json'\n",
    "    \n",
    "with open(\"input/lobstr_results.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "jata=pd.DataFrame(data['data'])\n",
    "jata[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input/lobstr_results.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "jata=pd.DataFrame(data['data'])\n",
    "jata[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lobstr via gdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://docs.google.com/spreadsheets/d/178sqEWzqubH0znhx7Z6u9ig2EjCRvl0dUsA7b6hQpmY/export?format=csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(url)\n",
    "df=df[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('input/lobstr_text.txt', sep='\\t', sep='\\t', index_col=0, names=['text'], skiprows=1)\n",
    "url = 'https://docs.google.com/spreadsheets/d/178sqEWzqubH0znhx7Z6u9ig2EjCRvl0dUsA7b6hQpmY/export?format=csv'\n",
    "df = pd.read_csv(url)\n",
    "len(articles) == len(df)\n",
    "# len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_parquet('input/lobstr_text.parquet', index=False)#, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj=pd.read_parquet('input/lobstr_text.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    dataloader = DataLoader(data['text'], batch_size=1, shuffle=True, num_workers=4)\n",
    "    RR = dataloader\n",
    "else:\n",
    "    RR = articles\n",
    "for j,i in tqdm(enumerate(RR), total=len(RR), desc=\"featurizing articles\"):\n",
    "\n",
    "# for i in tqdm(articles, desc=\"featurizing articles\"):\n",
    "    try:\n",
    "        foreparts = str(i).split(',')[:2]  # location and date\n",
    "    except:\n",
    "        foreparts=None\n",
    "    meat=\"\".join(str(j).split(',')[2:-3])  # text\n",
    "    try:\n",
    "        cc=featurize_stories(str(i), top_k = args.f, max_len=512)\n",
    "    except:\n",
    "        'damn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import requests, concurrent\n",
    "from bs4 import BeautifulSoup\n",
    "# def handler(signum, frame):\n",
    "    # raise TimeoutError()/\n",
    "# signal.signal(signal.SIGALRM, handler)\n",
    "\n",
    "\n",
    "def process_url(url):\n",
    "    try:\n",
    "        # signal.alarm(5)\n",
    "        response = requests.get(url,timeout=5)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all(['p'])\n",
    "        text = []\n",
    "        for p in paragraphs:\n",
    "            text.append(p.get_text())\n",
    "        # signal.alarm(0)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing URL {url}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[]\n",
    "# for i in tqdm(df['url'], desc=\"grabbing text from url\"):\n",
    "#     result = process_url(i)\n",
    "#     results.append(result)\n",
    "# df['text'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    result = list(tqdm(executor.map(process_url, df['url']), total=len(df['url'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(result, desc=\"featurizing articles\"):\n",
    "    foreparts = df[['published_at','title']]\n",
    "    try:\n",
    "        cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    except:\n",
    "        cc=None\n",
    "        pass\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rank_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(df['text'], desc=\"featurizing articles\"):\n",
    "    foreparts = df['datepublished_at','title'].tolist()\n",
    "    cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "hits[1]['_index']['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test-google-news-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/WRK/dcolinmorgan/dots'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DOTS.scrape import get_OS_data, get_test_gnews\n",
    "from DOTS.pull import pull_data, process_url\n",
    "from DOTS.ingestion_utils import safe_iter_pull, iter_pull, reduce_newlines, scrape_selenium_headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import subprocess, json, argparse, os,requests\n",
    "load_dotenv()\n",
    "os_url = os.getenv('OS_TOKEN')\n",
    "lobstr_key = os.getenv('LOBSTR_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "https://news.google.com/rss/articles/CBMihgFodHRwczovL3d3dy5hbmluZXdzLmluL25ld3Mvd29ybGQvbWlkZGxlLWVhc3QvODE0LWlzcmFlbGktY2l2aWxpYW5zLWtpbGxlZC1pbi1wYWxlc3Rpbmlhbi10ZXJyb3ItYXR0YWNrcy1zaW5jZS1vY3RvYmVyLTcyMDI0MDQwMjIxNDc1M9IBAA?oc=5\n",
      "https://news.google.com/rss/articles/CBMi5AFodHRwOi8vd3d3Lm1zbi5jb20vZW4teGwvYWZyaWNhL3RvcC1zdG9yaWVzL21vc2Nvdy1hdHRhY2stcnVzc2lhLWFycmVzdHMtbW9yZS1hdHRhY2stcGxvdHRlcnMvYXItQkIxa1NYVko_b2NpZD1YTU1PJmFwaXZlcnNpb249djImbm9zZXJ2ZXJjYWNoZT0xJmRvbXNoaW09MSZyZW5kZXJ3ZWJjb21wb25lbnRzPTEmd2NzZW89MSZiYXRjaHNlcnZlcnRlbGVtZXRyeT0xJm5vc2VydmVydGVsZW1ldHJ5PTHSAQA?oc=5\n",
      "https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vd3d3Lm1zbi5jb20vZW4temEvbmV3cy93b3JsZC93aHktdGhlLXVzLXdhcm5lZC1ydXNzaWEtb2YtYS10ZXJyb3Jpc3QtYXR0YWNrL2FyLUJCMWtWRXh50gEA?oc=5\n",
      "https://news.google.com/rss/articles/CBMibmh0dHBzOi8vd3d3LnJldXRlcnMuY29tL3dvcmxkL2V1cm9wZS9ydXNzaWEtc2F5cy1pdC1pcy13b3JraW5nLXJlbW92aW5nLXRhbGliYW4taXRzLXRlcnJvcmlzdC1saXN0LTIwMjQtMDQtMDIv0gEA?oc=5\n",
      "https://news.google.com/rss/articles/CBMidmh0dHBzOi8vd3d3Lm1zbi5jb20vZW4tdXMvbmV3cy93b3JsZC9ydXNzaWEtY29uZHVjdHMtY291bnRlci10ZXJyb3Jpc20tb3BlcmF0aW9uLWluLWRhZ2VzdGFuLWRldGFpbnMtdGhyZWUvYXItQkIxa09XZkPSAQA?oc=5\n",
      "https://news.google.com/rss/articles/CBMicmh0dHBzOi8vd3d3Lm1zbi5jb20vZW4taW4vbmV3cy93b3JsZC9pc3JhZWwtYm9tYmluZy1pbi1kYW1hc2N1cy1pcmFuLXZvd3MtcmV2ZW5nZS11LXMtY2xhaW1zLWlubm9jZW5jZS92aS1CQjFrVkpsUtIBAA?oc=5\n",
      "https://news.google.com/rss/articles/CBMieWh0dHBzOi8vd3d3LnBvc3QtZ2F6ZXR0ZS5jb20vbmV3cy93ZWF0aGVyLW5ld3MvMjAyNC8wNC8wMi9waXR0c2J1cmdoLXNldmVyZS13ZWF0aGVyLWZvcmVjYXN0LXVwZGF0ZXMvc3Rvcmllcy8yMDI0MDQwMjAwNTnSAQA?oc=5\n",
      "https://news.google.com/rss/articles/CBMiiAFodHRwczovL3d3dy5tc24uY29tL2VuLXVzL3dlYXRoZXIvdG9wc3Rvcmllcy9zcHJpbmctc3Rvcm0tdG8tYnJpbmctc25vdy10by11cHN0YXRlLW5ldy15b3JrLXBhcnRzLW9mLXRoZS10cmktc3RhdGUtdGhpcy13ZWVrL2FyLUJCMWtVZTFo0gEA?oc=5\n",
      "https://news.google.com/rss/articles/CBMihAFodHRwczovL3d3dy5zZWF0dGxldGltZXMuY29tL25hdGlvbi13b3JsZC9uYXRpb24vZmlyZWZpZ2h0ZXJzLXJlc2N1ZS0yLXBlb3BsZS10cmFwcGVkLXVuZGVyLW9oaW8tYnJpZGdlLWJ5LWZhc3QtcmlzaW5nLXJpdmVyLXdhdGVycy_SAQA?oc=5\n",
      "https://news.google.com/rss/articles/CBMigQFodHRwczovL3d3dy5tc24uY29tL2VuLWllL25ld3Mvd29ybGQvbWlsaXRhcnktbGVhZGVyLWFtb25nLXRob3NlLWtpbGxlZC1pbi1pc3JhZWxpLXN0cmlrZS1vbi1pcmFuaWFuLWVtYmFzc3ktaW4tc3lyaWEvYXItQkIxa1VrOHXSAQA?oc=5\n",
      "https://news.google.com/rss/articles/CBMiTWh0dHBzOi8vd3d3LndoaWRiZXluZXdzdGltZXMuY29tL25ld3MvbWFyaW5hLXBvd2VyLW91dGFnZS1kZWNsYXJlZC1lbWVyZ2VuY3kv0gEA?oc=5\n",
      "https://news.google.com/rss/articles/CBMicmh0dHBzOi8vYXNpYXBhY2lmaWNyZXBvcnQubnovMjAyNC8wNC8wMi9iYWx0aW1vcmUtYnJpZGdlLWNyYXNoLXNoaXAtY2FycnlpbmctdG94aWMtd2FzdGUtdG8tc3JpLWxhbmthLXNheXMtbWlycm9yL9IBAA?oc=5\n",
      "https://news.google.com/rss/articles/CBMiamh0dHBzOi8vd3d3LmJlZGZvcmR0b2RheS5jby51ay9uZXdzL2hhbmQtZ3JlbmFkZS1zYWZlbHktZGV0b25hdGVkLWFmdGVyLWRpc2NvdmVyeS1pbi1iZWRmb3JkLXJpdmVyLTQ1NzYxMjfSAQA?oc=5\n",
      "https://news.google.com/rss/articles/CBMiZGh0dHBzOi8vcGVyc3BlY3RpdmVzbWVkLmNvbS91bi1pc3JhZWxpZW4tYXJyZXRlLWEta3VhbGEtbHVtcHVyLXZ1bGdhaXJlLWNyaW1pbmVsLW91LWFnZW50LWR1LW1vc3NhZC_SAQA?oc=5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# response = get_test_gnews(100)\n",
    "# hits = response[\"hits\"][\"hits\"]\n",
    "# articles = pull_data(hits)\n",
    "# indices = [i for i, x in enumerate(articles) if len(str(x)) < 50]\n",
    "print(len(indices))\n",
    "for i in indices:\n",
    "    url = hits[i]['_source']['metadata']['link']\n",
    "    print(url)\n",
    "    try:\n",
    "        articles[i] = scrape_selenium_headless(url,browser='undetected_chrome')\n",
    "    except:\n",
    "        pass\n",
    "indices2 = [i for i, x in enumerate(articles) if len(str(x)) <50]\n",
    "print(len(indices2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "indices2 = [i for i, x in enumerate(articles) if len(str(x)) <50]\n",
    "print(len(indices2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent_list = [ 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36', 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_4_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1', 'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18363', ]\n",
    "scrapy.Request(url, callback=self.parse, headers={ \"User-Agent\": self.user_agent_list[random.randint(0, len(self.user_agent_list) - 1)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/apple/WRK/dcolinmorgan/dots/DOTS/msn_scraper/')\n",
    "\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from msn_scraper.spiders.msn_spider import msn_spider\n",
    "\n",
    "# process = CrawlerProcess({\n",
    "#     'USER_AGENT': 'Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "# })\n",
    "\n",
    "process.crawl(msn_spider)\n",
    "process.start()  # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 16:40:50 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-04-08 16:40:50 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.9.0, w3lib 2.1.2, Twisted 24.3.0, Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:54:21) [Clang 16.0.6 ], pyOpenSSL 24.1.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform macOS-14.4.1-arm64-arm-64bit\n",
      "2024-04-08 16:40:50 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-04-08 16:40:50 [py.warnings] WARNING: /opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-04-08 16:40:50 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-04-08 16:40:50 [scrapy.extensions.telnet] INFO: Telnet Password: ed44e936a2e4e2fa\n",
      "2024-04-08 16:40:50 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-04-08 16:40:50 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-04-08 16:40:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-04-08 16:40:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-04-08 16:40:50 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-04-08 16:40:50 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-04-08 16:40:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-04-08 16:40:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6028\n",
      "2024-04-08 16:40:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://news.google.com/rss/articles/CBMiZGh0dHBzOi8vcGVyc3BlY3RpdmVzbWVkLmNvbS91bi1pc3JhZWxpZW4tYXJyZXRlLWEta3VhbGEtbHVtcHVyLXZ1bGdhaXJlLWNyaW1pbmVsLW91LWFnZW50LWR1LW1vc3NhZC_SAQA?oc=5&hl=en-US&gl=US&ceid=US:en> from <GET https://news.google.com/rss/articles/CBMiZGh0dHBzOi8vcGVyc3BlY3RpdmVzbWVkLmNvbS91bi1pc3JhZWxpZW4tYXJyZXRlLWEta3VhbGEtbHVtcHVyLXZ1bGdhaXJlLWNyaW1pbmVsLW91LWFnZW50LWR1LW1vc3NhZC_SAQA?oc=5>\n",
      "2024-04-08 16:40:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://news.google.com/rss/articles/CBMiZGh0dHBzOi8vcGVyc3BlY3RpdmVzbWVkLmNvbS91bi1pc3JhZWxpZW4tYXJyZXRlLWEta3VhbGEtbHVtcHVyLXZ1bGdhaXJlLWNyaW1pbmVsLW91LWFnZW50LWR1LW1vc3NhZC_SAQA?oc=5&hl=en-US&gl=US&ceid=US:en> (referer: None)\n",
      "2024-04-08 16:40:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://news.google.com/rss/articles/CBMiZGh0dHBzOi8vcGVyc3BlY3RpdmVzbWVkLmNvbS91bi1pc3JhZWxpZW4tYXJyZXRlLWEta3VhbGEtbHVtcHVyLXZ1bGdhaXJlLWNyaW1pbmVsLW91LWFnZW50LWR1LW1vc3NhZC_SAQA?oc=5&hl=en-US&gl=US&ceid=US:en> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/utils/defer.py\", line 279, in iter_errback\n",
      "    yield next(it)\n",
      "          ^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/spidermiddlewares/offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/var/folders/sx/x954rbdd44d932dd0ygfp1qc0000gn/T/ipykernel_21985/4249395762.py\", line 36, in parse\n",
      "    f.write(text)\n",
      "            ^^^^\n",
      "NameError: name 'text' is not defined. Did you mean: 'next'?\n",
      "2024-04-08 16:40:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-04-08 16:40:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1215,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 91786,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 0.704958,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 4, 8, 8, 40, 51, 277384, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 276872,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 111706112,\n",
      " 'memusage/startup': 111706112,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'spider_exceptions/NameError': 1,\n",
      " 'start_time': datetime.datetime(2024, 4, 8, 8, 40, 50, 572426, tzinfo=datetime.timezone.utc)}\n",
      "2024-04-08 16:40:51 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import random, scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class msn_spider(scrapy.Spider):\n",
    "    name = \"msn\"\n",
    "\n",
    "    start_urls = [\n",
    "        'https://news.google.com/rss/articles/CBMiZGh0dHBzOi8vcGVyc3BlY3RpdmVzbWVkLmNvbS91bi1pc3JhZWxpZW4tYXJyZXRlLWEta3VhbGEtbHVtcHVyLXZ1bGdhaXJlLWNyaW1pbmVsLW91LWFnZW50LWR1LW1vc3NhZC_SAQA?oc=5',\n",
    "    ]\n",
    "    user_agent_list = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36',\n",
    "        'Mozilla/5.0 (iPhone; CPU iPhone OS 14_4_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1',\n",
    "        'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18363',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36',\n",
    "        'Mozilla/5.0 (iPhone; CPU iPhone OS 14_4_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1',\n",
    "        'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18363', \n",
    "    ]\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, headers={'User-Agent': random.choice(self.user_agent_list)})\n",
    "\n",
    "    def parse(self, response):\n",
    "        html = response.text\n",
    "        # for paragraph in response.css('p'):\n",
    "            # yield {'text': paragraph.get()}\n",
    "        # soup = BeautifulSoup(html, 'html.parser')\n",
    "        # see = reduce_newlines(soup.get_text())\n",
    "        with open('output.txt', 'a') as f:\n",
    "            f.write(html)\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl(msn_spider)\n",
    "process.start()  # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: disconnected: unable to send message to renderer\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=123.0.6312.107)\nStacktrace:\n0   chromedriver                        0x0000000102950474 chromedriver + 4326516\n1   chromedriver                        0x000000010294893c chromedriver + 4294972\n2   chromedriver                        0x0000000102574088 chromedriver + 278664\n3   chromedriver                        0x000000010255d468 chromedriver + 185448\n4   chromedriver                        0x000000010255bc5c chromedriver + 179292\n5   chromedriver                        0x000000010255c930 chromedriver + 182576\n6   chromedriver                        0x0000000102577078 chromedriver + 290936\n7   chromedriver                        0x00000001025778c0 chromedriver + 293056\n8   chromedriver                        0x000000010257a428 chromedriver + 304168\n9   chromedriver                        0x000000010257a4a0 chromedriver + 304288\n10  chromedriver                        0x00000001025b6b7c chromedriver + 551804\n11  chromedriver                        0x00000001025ef4f8 chromedriver + 783608\n12  chromedriver                        0x00000001025ab4e4 chromedriver + 505060\n13  chromedriver                        0x00000001025abf5c chromedriver + 507740\n14  chromedriver                        0x0000000102913a10 chromedriver + 4078096\n15  chromedriver                        0x00000001029187c8 chromedriver + 4097992\n16  chromedriver                        0x00000001028fa5b4 chromedriver + 3974580\n17  chromedriver                        0x00000001029190e0 chromedriver + 4100320\n18  chromedriver                        0x00000001028ebba4 chromedriver + 3914660\n19  chromedriver                        0x00000001029396e8 chromedriver + 4232936\n20  chromedriver                        0x0000000102939864 chromedriver + 4233316\n21  chromedriver                        0x00000001029485b0 chromedriver + 4294064\n22  libsystem_pthread.dylib             0x000000019462ef94 _pthread_start + 136\n23  libsystem_pthread.dylib             0x0000000194629d34 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url_main)\n\u001b[1;32m     35\u001b[0m driver\u001b[38;5;241m.\u001b[39mimplicitly_wait(\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m element_has_bottom_message \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas-bottom-messaging\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m element_has_bottom_message:\n\u001b[1;32m     38\u001b[0m     element_gdpr \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m120\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[1;32m     39\u001b[0m         EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgdpr-banner__accept\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/selenium/webdriver/support/expected_conditions.py:84\u001b[0m, in \u001b[0;36mpresence_of_element_located.<locals>._predicate\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: disconnected: unable to send message to renderer\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=123.0.6312.107)\nStacktrace:\n0   chromedriver                        0x0000000102950474 chromedriver + 4326516\n1   chromedriver                        0x000000010294893c chromedriver + 4294972\n2   chromedriver                        0x0000000102574088 chromedriver + 278664\n3   chromedriver                        0x000000010255d468 chromedriver + 185448\n4   chromedriver                        0x000000010255bc5c chromedriver + 179292\n5   chromedriver                        0x000000010255c930 chromedriver + 182576\n6   chromedriver                        0x0000000102577078 chromedriver + 290936\n7   chromedriver                        0x00000001025778c0 chromedriver + 293056\n8   chromedriver                        0x000000010257a428 chromedriver + 304168\n9   chromedriver                        0x000000010257a4a0 chromedriver + 304288\n10  chromedriver                        0x00000001025b6b7c chromedriver + 551804\n11  chromedriver                        0x00000001025ef4f8 chromedriver + 783608\n12  chromedriver                        0x00000001025ab4e4 chromedriver + 505060\n13  chromedriver                        0x00000001025abf5c chromedriver + 507740\n14  chromedriver                        0x0000000102913a10 chromedriver + 4078096\n15  chromedriver                        0x00000001029187c8 chromedriver + 4097992\n16  chromedriver                        0x00000001028fa5b4 chromedriver + 3974580\n17  chromedriver                        0x00000001029190e0 chromedriver + 4100320\n18  chromedriver                        0x00000001028ebba4 chromedriver + 3914660\n19  chromedriver                        0x00000001029396e8 chromedriver + 4232936\n20  chromedriver                        0x0000000102939864 chromedriver + 4233316\n21  chromedriver                        0x00000001029485b0 chromedriver + 4294064\n22  libsystem_pthread.dylib             0x000000019462ef94 _pthread_start + 136\n23  libsystem_pthread.dylib             0x0000000194629d34 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "capabilities = DesiredCapabilities().CHROME\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-infobars\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "\n",
    "# disable the banner \"Chrome is being controlled by automated test software\"\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "\n",
    "# driver = webdriver.Chrome('/usr/local/bin/chromedriver', options=chrome_options)\n",
    "webdriver_service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "url_main = 'https://www.scmp.com/news/asia/east-asia/article/3199400/japan-asean-hold-summit-tokyo-around-december-2023-japanese-official'\n",
    "# url_main = 'https://news.google.com/rss/articles/CBMiZGh0dHBzOi8vcGVyc3BlY3RpdmVzbWVkLmNvbS91bi1pc3JhZWxpZW4tYXJyZXRlLWEta3VhbGEtbHVtcHVyLXZ1bGdhaXJlLWNyaW1pbmVsLW91LWFnZW50LWR1LW1vc3NhZC_SAQA?oc=5'\n",
    "\n",
    "driver.get(url_main)\n",
    "\n",
    "driver.implicitly_wait(20)\n",
    "element_has_bottom_message = WebDriverWait(driver, 120).until(EC.presence_of_element_located((By.CLASS_NAME, \"has-bottom-messaging\")))\n",
    "if element_has_bottom_message:\n",
    "    element_gdpr = WebDriverWait(driver, 120).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"gdpr-banner__accept\")))\n",
    "    if element_gdpr:\n",
    "        gdrp_button = driver.find_element_by_xpath(\"//*[@class='gdpr-banner__accept']\")\n",
    "        driver.implicitly_wait(20)\n",
    "        ActionChains(driver).move_to_element(gdrp_button).click(gdrp_button).perform()\n",
    "        element_my_news_popup = WebDriverWait(driver, 120).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"my-news-landing-popup__icon-close\")))\n",
    "        if element_my_news_popup:\n",
    "            my_news_popup = driver.find_element_by_xpath(\"//*[@class='my-news-landing-popup__icon-close']\")\n",
    "            ActionChains(driver).move_to_element(my_news_popup).click(my_news_popup).perform()\n",
    "            driver.implicitly_wait(20)\n",
    "            raw_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            json_dictionaries = raw_soup.find_all(name='script', attrs={'type': 'application/ld+json'})\n",
    "            if len(json_dictionaries) != 0:\n",
    "                for json_dictionary in json_dictionaries:\n",
    "                    dictionary = json.loads(\"\".join(json_dictionary.contents), strict=False)\n",
    "                    article_bool = bool([value for (key, value) in dictionary.items() if key == 'articleBody'])\n",
    "                    if article_bool:\n",
    "                        for key, value in dictionary.items():\n",
    "                            if key == 'articleBody':\n",
    "                                print(value)\n",
    "\n",
    "\n",
    "sleep(30)\n",
    "driver.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the spider\n",
    "spider = msn_spider()\n",
    "\n",
    "# Call the start_requests function and store the returned list in a variable\n",
    "requests = spider.start_requests()\n",
    "\n",
    "# Print the requests\n",
    "for request in requests:\n",
    "    print(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DOTS.feat import featurize_stories\n",
    "import graphistry\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurize_stories(str(articles[12]), top_k = 3, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_articles=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(articles))):\n",
    "    try:\n",
    "        cc = featurize_stories(str(articles[i]), top_k = 3, max_len=512)\n",
    "        rank_articles.append([cc])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in rank_articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.DataFrame(flattened_list)  # each ranked feature is a row\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# g = graphistry.edges(data).materialize_nodes()\n",
    "g = graphistry.nodes(data)\n",
    "g2 = g.featurize()\n",
    "# g2._clustersummary()\n",
    "\n",
    "g3 = g.umap()\n",
    "g3.dbscan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.encode_point_color('_dbscan',palette=[\"hotpink\", \"dodgerblue\"],as_continuous=True).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcoup scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get('https://funcoup.org/downloads/')\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the links on the page\n",
    "links = soup.find_all('a')\n",
    "text = soup.get_text()\n",
    "\n",
    "# Filter the links to only include those that end with .gz\n",
    "gz_links = [link for link in links if link['href'].endswith('.gz')]\n",
    "sizes = re.findall(r'(\\d+\\.\\d+|\\d+)(?=\\s*(MB|KB))', text)\n",
    "\n",
    "# For each .gz link, extract the file name and size\n",
    "species=[]\n",
    "for link, size in zip(gz_links, sizes):\n",
    "    file_name = link.text\n",
    "    file_size = size\n",
    "    species.append([file_name, file_size])\n",
    "species=pd.DataFrame(species, columns=['file_name', 'file_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_kb(size):\n",
    "    value, unit = size\n",
    "    value = float(value)\n",
    "    if unit == 'MB':\n",
    "        value *= 1024  # convert MB to KB\n",
    "    return value\n",
    "\n",
    "# Add a new column 'type' based on the content of 'file_name'\n",
    "species['type'] = species['file_name'].apply(lambda x: 'full' if 'full' in x else ('compact' if 'compact' in x else 'unknown'))\n",
    "\n",
    "# Convert sizes to KB and sort\n",
    "species['file_size_kb'] = species['file_size'].apply(convert_to_kb)\n",
    "species = species.sort_values(by='file_size_kb', ascending=True)\n",
    "\n",
    "# Drop the 'file_size_kb' column if you don't need it\n",
    "species = species.drop(columns=['file_size_kb'])\n",
    "species['name']=species['file_name'].str.split('_').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'file_name' column to a dictionary\n",
    "file_name_dict = species['name'].to_dict()\n",
    "\n",
    "# Make a dictionary where the key and value are the same for each row\n",
    "name_dict = {v: v for k, v in file_name_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a list\n",
    "name_list = list(name_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(name_dict).replace('{', '(').replace('}', ')').replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
