{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run model on sample of OpenSearch tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "def is_installed(package):\n",
    "    try:\n",
    "        pkg_resources.get_distribution(package)\n",
    "        return True\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False\n",
    "\n",
    "print(is_installed('cudf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "# import concurrent.futures\n",
    "# from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch, spacy,nltk,subprocess, json, requests,string,csv\n",
    "\n",
    "model_name = \"distilroberta-base\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "n_gram_range = (1, 2)\n",
    "stop_words = \"english\"\n",
    "embeddings=[]\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "import nltk, string, numpy as np\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npr_news(p):\n",
    "    # Send a GET request to the NPR API\n",
    "    r = requests.get(\"http://api.npr.org/query?apiKey=npr_key\", params=p)\n",
    "\n",
    "    # Parse the XML response to get the story URLs\n",
    "    root = ET.fromstring(r.content)\n",
    "    story_urls = [story.find('link').text for story in root.iter('story')]\n",
    "\n",
    "    # For each story URL, send a GET request to get the HTML content\n",
    "    full_stories = []\n",
    "    for url in story_urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content of the story. This will depend on the structure of the webpage.\n",
    "        # Here, we're assuming that the main content is in a <p> tag. You might need to adjust this depending on the webpage structure.\n",
    "        story = soup.find_all('p')\n",
    "\n",
    "        # Extract the text from the story\n",
    "        full_story = ' '.join(p.text for p in story)\n",
    "        full_stories.append(full_story)\n",
    "    return full_stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_len):\n",
    "    # Tokenize the text into tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Calculate the number of chunks and the size of the final chunk\n",
    "    num_chunks = len(tokens) // max_len\n",
    "    final_chunk_size = len(tokens) % max_len\n",
    "\n",
    "    # If the final chunk is too small, distribute its tokens among the other chunks\n",
    "    if final_chunk_size < max_len / 2:\n",
    "        num_chunks += 1\n",
    "        chunk_sizes = [len(tokens) // num_chunks + (1 if i < len(tokens) % num_chunks else 0) for i in range(num_chunks)]\n",
    "        chunks = [tokens[sum(chunk_sizes[:i]):sum(chunk_sizes[:i+1])] for i in range(num_chunks)]\n",
    "    else:\n",
    "        chunks = [tokens[i:i + max_len] for i in range(0, len(tokens), max_len)]\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def featurize_stories(text, max_len, top_k):\n",
    "    # Extract candidate words/phrases\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "    all_candidates = count.get_feature_names_out()\n",
    "    doc = nlp(text)\n",
    "    noun_phrases = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
    "    nouns = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.add(token.text)\n",
    "\n",
    "    all_nouns = nouns.union(noun_phrases)\n",
    "    candidates = list(filter(lambda candidate: candidate in all_nouns, all_candidates))\n",
    "    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=\"pt\")\n",
    "    # candidate_tokens = {k: v.to(device) for k, v in (candidate_tokens).items()}\n",
    "    candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
    "    candidate_embeddings = candidate_embeddings.detach()#.to_numpy()\n",
    "\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    # chunks = [words[i:i + 512] for i in range(0, len(words), 512)]\n",
    "    chunks = chunk_text(text, max_len)  # use this to chunk better and use less padding thus less memory but also less affect from averging\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text_tokens = tokenizer(chunk, padding=True, return_tensors=\"pt\")\n",
    "        text_tokens = {k: v.to(device) for k, v in (text_tokens).items()}\n",
    "        text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
    "        text_embedding = text_embedding.detach()#.to_numpy()\n",
    "        embeddings.append(text_embedding)\n",
    "    max_emb_shape = max(embedding.shape[0] for embedding in embeddings)\n",
    "    padded_embeddings = [np.pad(embedding.cpu(), ((0, max_emb_shape - embedding.shape[0]), (0, 0))) for embedding in embeddings]\n",
    "    avg_embedding = np.min(padded_embeddings, axis=0)\n",
    "    distances = cosine_similarity(avg_embedding, candidate_embeddings.cpu())\n",
    "    torch.cuda.empty_cache()\n",
    "    return [candidates[index] for index in distances.argsort()[0][::-1][-top_k:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# data=pd.read_csv('/content/drive/MyDrive/consult/Louie_disaster_tweets.csv',header=None)\n",
    "data=pd.read_csv('florida-hurricane-tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('8.5k_florida_tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_articles=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(df),len(data['text']))):\n",
    "    try:\n",
    "        cc=featurize_stories(data['text'][i], max_len=512, top_k=4)\n",
    "        # print(cc)\n",
    "        rank_articles.append(cc)\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in rank_articles for item in sublist]\n",
    "from collections import Counter\n",
    "counter = Counter(flattened_list)\n",
    "df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n",
    "\n",
    "df = df.sort_values(by='Count',ascending=False)\n",
    "df.to_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "print(len(df))\n",
    "# df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [sublist for sublist in rank_articles if any('fire'.lower() in s.lower() for s in sublist)]\n",
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('florida-hurricane-tweet_features.txt',sep='\\t')\n",
    "df2=pd.read_csv('florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "df=pd.concat([df,df2])\n",
    "print(df['Unnamed: 0'])\n",
    "df = df.groupby('Unnamed: 0').sum().sort_values(by='Count',ascending=False)\n",
    "df=df[df['Count']>int(np.round(len(df)*.001))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# nouns = ['apple', 'John', 'London', 'dog', 'Mary', 'Paris', 'banana']\n",
    "nouns= df.reset_index()['Unnamed: 0'].to_list()\n",
    "doc = nlp(' '.join(nouns))\n",
    "\n",
    "proper_nouns = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "print(proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(proper_nouns))\n",
    "proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=['Iceland', '16-01-2024', ['Lava from a volcanic eruption in southwestern Iceland has streamed into a nearby town, engulfing homes and forcing the evacuation of local residents.\\n', 'The Icelandic Meteorological Office said the eruption began just before 8:00 a.m. local time Sunday about a half mile from the town of Grindavík after a series of intense earthquakes.\\n', 'A second fissure opened after noon Sunday and sent lava flows into the town, officials said.\\n', 'Iceland\\'s president, Guðni Th. Jóhannesson, said in an address to the nation on Sunday that a \"daunting period of upheaval\" had begun for those in the Reykjanes peninsula.\\n', '\"We continue to hope for as good an outcome as possible, in the face of these tremendous forces of nature,\" he said.\\n', 'This is the second time in a month that a volcano has erupted just outside Grindavík, a coastal town about 25 miles from the Icelandic capital of Reykjavík.\\n', 'An eruption on December 18 sent lava spewing into the air, but residents had already been told to leave due to heightened seismic activity.\\n', \"Iceland's government said Sunday that Grindavík had been evacuated early that morning and that the eruption isn't expected to reach other populated areas.\\n\", 'So far no flights have been disrupted, and officials are monitoring threats to infrastructure. At least three homes have either burned down or been overtaken by lava, according to the Icelandic broadcaster RUV.\\n', \"The government also said the eruption doesn't present a threat to life.\\n\", 'But in his speech, Jóhannesson offered his sympathy to the loved ones of Lúðvík Pétursson, a man who went missing in a work accident in Grindavík last week.\\n', \"According to Sky News, the 50-year-old Pétursson was filling crevasses formed by volcanic activity and earthquakes when he fell in a crack that had opened after last month's eruption.\\n\", 'Iceland is a hotspot for seismic activity, with 32 active volcanoes. A volcano erupts roughly every five years in the country, though eruptions have occurred more frequently recently. \\n', ' Copyright 2024 NPR. To see more, visit https://www.npr.org.  ', '\\n'], None, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameterize functions\n",
    " - parallelize text pulls from urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, signal,datetime,subprocess,json,os\n",
    "from dotenv import load_dotenv\n",
    "parser = argparse.ArgumentParser(description='Process OS data for dynamic features.')\n",
    "parser.add_argument('-n', type=int, default=10, help='Number of data items to get')\n",
    "parser.add_argument('-f', type=int, default=3, help='Number of features per item to get')\n",
    "parser.add_argument('-o', type=str, default='dots_feats.csv', help='Output file name')\n",
    "parser.add_argument('-p', type=int, default=1, help='Parallelize requests')\n",
    "# parser.add_argument('-s', type=datetime, default=20230101, help='start date')\n",
    "# parser.add_argument('-e', type=datetime, default=20231231, help='end date')\n",
    "args, unknown = parser.parse_known_args()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import get_OS_data\n",
    "dd=get_OS_data(10)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os_url=([os.environ['OS_TOKEN']])\n",
    "\n",
    "os_url = os.getenv('OS_TOKEN')\n",
    "n=10\n",
    "bash_command = f\"\"\"\n",
    "curl -X GET \"{os_url}/emergency-management-news/_search?scroll=1m\" -H 'Content-Type: application/json' -d '{{\n",
    "\"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "    \"size\": {n},\n",
    "    \"query\": {{\n",
    "        \"bool\": {{\n",
    "            \"must\": [\n",
    "                {{\"match_all\": {{}}}}\n",
    "            ]\n",
    "        }}\n",
    "    }}\n",
    "}}'\n",
    "\"\"\"\n",
    "process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "output = process.stdout\n",
    "data = json.loads(output)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.loads(\"input/feat_input.json\")\n",
    "with open(\"input/feat_input.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "data['_scroll_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the scroll ID\n",
    "scroll_id = response['_scroll_id']\n",
    "es = OpenSearch([os.environ['OS_TOKEN']])\n",
    "# Keep scrolling until no more results\n",
    "while len(response['hits']['hits']):\n",
    "    # print(response['hits']['hits'])  # process results\n",
    "\n",
    "    response = es.scroll(\n",
    "        scroll_id=scroll_id,\n",
    "        scroll='1m'  # length of time to keep the scroll window open\n",
    "    )\n",
    "\n",
    "    # Update the scroll ID\n",
    "    scroll_id = response['_scroll_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import featurize_stories, process_data, get_OS_data, process_hit\n",
    "data = get_OS_data(10)\n",
    "data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,argparse,datetime\n",
    "sys.argv = ['dots_feat.py', '-n', '10', '-f', '3', '-o', 'dots_feats.csv', '-p', '1']#, '-s', '20230101', '-e', '20231231']\n",
    "parser = argparse.ArgumentParser(description='Process OS data for dynamic features.')\n",
    "parser.add_argument('-n', type=int, default=10, help='Number of data items to get')\n",
    "parser.add_argument('-f', type=int, default=3, help='Number of features per item to get')\n",
    "parser.add_argument('-o', type=str, default='dots_feats.csv', help='Output file name')\n",
    "parser.add_argument('-p', type=int, default=1, help='Parallelize requests')\n",
    "# parser.add_argument('-s', type=datetime.date, default=datetime.datetime.strptime(20230101), help='start date')\n",
    "# parser.add_argument('-e', type=datetime.date, default=datetime.datetime.strptime(20231231), help='end date')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "from dots.dots_feat import featurize_stories, process_data, get_OS_data, process_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"20230101\"\n",
    "e=\"20231231\"\n",
    "\n",
    "s_date = datetime.datetime.strptime(s, \"%Y%m%d\")\n",
    "e_date = datetime.datetime.strptime(e, \"%Y%m%d\")\n",
    "\n",
    "# Convert datetime objects to ISO 8601 format\n",
    "s_iso = s_date.isoformat() + \"Z\"\n",
    "e_iso = e_date.isoformat() + \"Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s_iso,e_iso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OS_data(n=args.n):\n",
    "    bash_command = f\"\"\"\n",
    "    curl -X GET \"{os_url}\" -H 'Content-Type: application/json' -d '{{\n",
    "\"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "        \"size\": {n},\n",
    "        \"query\": {{\n",
    "            \"bool\": {{\n",
    "                \"must\": [\n",
    "                    {{\"match_all\": {{}}}},\n",
    "                ]\n",
    "            }}\n",
    "        }}\n",
    "    }}'\n",
    "    \"\"\"\n",
    "    process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "    output = process.stdout\n",
    "    data = json.loads(output)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "hits = data['hits']['hits']\n",
    "hit = hits[1]\n",
    "hit['_source']\n",
    "\n",
    "from datetime import datetime\n",
    "source = hit['_source']\n",
    "date = datetime.strptime(source['metadata']['GDELT_DATE'], \"%Y%m%d%H%M%S\")\n",
    "loc = source['metadata']['Locations']\n",
    "# ex = None #source['metadata']['Extras']\n",
    "title = source['metadata']['page_title']\n",
    "url = source['metadata']['DocumentIdentifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = source['metadata']['Locations']\n",
    "\n",
    "s = s.replace(\"'\", '\"')  # json requires double quotes for keys and string values\n",
    "list_of_dicts = json.loads(s)\n",
    "\n",
    "\n",
    "# location_full_name = source['metadata']['Locations']['Location FullName']\n",
    "\n",
    "for dict in list_of_dicts:\n",
    "    if 'Location FullName' in dict:\n",
    "        print(dict['Location FullName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for text, soup,date,loc,title in results:\n",
    "            articles.append([loc,loc,loc,date,title,title,text])\n",
    "            # articles.append(text)\n",
    "\n",
    "            writer.writerow([loc,loc,loc,date,title,title,text]) # force location into top feature, also assume title has important info\n",
    "            # writer.writerow(text)\n",
    "            writer.writerow(['\\n'])\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import process_data_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hits(hit):\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get {url}\")\n",
    "        return z\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all(['p','div','a'])\n",
    "    if not paragraphs:\n",
    "        print(f\"No <p> tags in {url}\")\n",
    "        return z\n",
    "\n",
    "    for p in paragraphs:\n",
    "        z.append(p.get_text())\n",
    "    return z\n",
    "\n",
    "with open('output.html', 'w') as file:\n",
    "    file.write(str(soup))\n",
    "    \n",
    "with open('output.csv', 'w') as file:\n",
    "    file.write(str(z))\n",
    "\n",
    "# return articles\n",
    "articles=[]\n",
    "hits = data['hits']['hits']\n",
    "for hit in hits:\n",
    "    z=process_hits(hit)\n",
    "    articles.append(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_hit(hit):\n",
    "    z = []\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    location = source['metadata'][\"Locations\"]\n",
    "    extras = source['metadata'][\"Extras\"]\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for p in soup.find_all('p'):\n",
    "        z.append(p.get_text())\n",
    "    z.append(page_title)\n",
    "    return z, soup\n",
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for z, soup in results:\n",
    "            writer.writerow(z)\n",
    "            writer.writerow(['\\n'])\n",
    "            articles.append(z)\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "# articles = process_data(data)\n",
    "hits = data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "results=[]\n",
    "e = concurrent.futures.ThreadPoolExecutor()\n",
    "    # try:\n",
    "future = e.submit(process_hit, hits[1])\n",
    "result = future.result(timeout=5)  # Set timeout for 5 seconds\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hits']['hits'][1]['_source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests,os,csv,logging\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(response):\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    output=[]\n",
    "    for iii,hit in enumerate(hits):\n",
    "\n",
    "        source = hit[\"_source\"]\n",
    "        text=[]\n",
    "        try:\n",
    "            date = datetime.strptime(source['metadata']['GDELT_DATE'], \"%Y%m%d%H%M%S\")\n",
    "            date = formatted_date = date.strftime(\"%d-%m-%Y\")\n",
    "            loc = source['metadata']['Locations']\n",
    "            loc = loc.replace(\"'\", '\"')  # json requires double quotes for keys and string values\n",
    "            try:\n",
    "                list_of_dicts = json.loads(loc)\n",
    "                location_full_names = [dict['Location FullName'] for dict in list_of_dicts if 'Location FullName' in dict]\n",
    "                loc = location_full_names[0]\n",
    "            except:\n",
    "                loc = None\n",
    "            org = source['metadata']['Organizations']\n",
    "            per = source['metadata']['Persons']\n",
    "            theme = source['metadata']['Themes'].rsplit('_')[-1]\n",
    "            title = source['metadata']['page_title']\n",
    "            url = source['metadata']['DocumentIdentifier']\n",
    "            output.append([date, loc, title, org, per, theme, url])\n",
    "            # pagination_id=response['_scroll_id']\n",
    "        \n",
    "        except:\n",
    "            # pagination_id=None\n",
    "            pass\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, client = get_massive_OS_data(args.n,args.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response['hits']['hits']\n",
    "# response[\"_scroll_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=[]\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "while len(hits) != 0:\n",
    "    # try:\n",
    "    response = client.scroll(\n",
    "        scroll='5m',\n",
    "        scroll_id=pagination_id\n",
    "            )\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    article = process_response(response)\n",
    "    articles.append(article)\n",
    "    # except:\n",
    "    #     print(\"A ConnectionTimeout error occurred.\")\n",
    "    #     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flattened_list = [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scroll OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()\n",
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scrape import get_OS_data, get_massive_OS_data,get_google_news\n",
    "from pull import process_hit, process_data,pull_data\n",
    "os.getcwd()\n",
    "# os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scroll_data():\n",
    "    response, client = get_massive_OS_data(1)\n",
    "    pagination_id = response[\"_scroll_id\"]\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    articles=[]\n",
    "    articles2=[]\n",
    "    while len(hits) != 0 and len(articles2) < 11000:\n",
    "        response = client.scroll(\n",
    "            scroll='1m',\n",
    "            scroll_id=pagination_id\n",
    "                )\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        # article = process_data(response)\n",
    "        articles.append(hits)\n",
    "        articles2 = [item for sublist in articles for item in sublist]\n",
    "    return [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(location_str):\n",
    "    if location_str:\n",
    "        try:\n",
    "            location_list = json.loads(location_str.replace(\"'\", '\"'))\n",
    "            return [dict['Location FullName'] for dict in location_list if 'Location FullName' in dict]\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all(['p'])\n",
    "    text = []\n",
    "    for p in paragraphs:\n",
    "        text.append(p.get_text())\n",
    "    return text\n",
    "\n",
    "import json\n",
    "import concurrent.futures, requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [list(d['_source']['metadata'].values()) for d in articles]\n",
    "df = pd.DataFrame(data, columns=['date','title', 'person', 'org', 'location', 'theme', 'text', 'url'])\n",
    "df.date=pd.to_datetime(df.date).dt.strftime('%d-%m-%Y')\n",
    "df['locc'] = df['location'].apply(extract_location)\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    df['text'] = list(executor.map(process_url, df['url']))\n",
    "return df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## small gnews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "from tqdm import tqdm\n",
    "from scrape import get_OS_data, get_massive_OS_data,get_google_news\n",
    "from pull import process_hit, process_data,pull_data\n",
    "# from feat import featurize_stories\n",
    "google_news = GNews()\n",
    "\n",
    "# google_news.period = '7d'  # News from last 7 days\n",
    "google_news.max_results = 10000  # number of responses across a keyword\n",
    "google_news.country = 'United States'  # News from a specific country \n",
    "google_news.language = 'english'  # News in a specific language\n",
    "google_news.exclude_websites = ['yahoo.com', 'cnn.com']  # Exclude news from specific website i.e Yahoo.com and CNN.com\n",
    "google_news.start_date = (2024, 1, 1) # Search from 1st Jan 2020\n",
    "google_news.end_date = (2024, 3, 1) # Search until 1st March 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_resp = google_news.get_news('disaster')\n",
    "article=[]\n",
    "\n",
    "for i in tqdm(range(len(json_resp)), desc=\"grabbing directly from GoogleNews\"):\n",
    "    aa=(google_news.get_full_article(json_resp[i]['url']))\n",
    "    try:\n",
    "        date=aa.publish_date.strftime(\"%d-%m-%Y\")\n",
    "    except:\n",
    "        date=None\n",
    "    try:\n",
    "        title=aa.title\n",
    "        text=aa.text\n",
    "    except:\n",
    "        title=None\n",
    "        text=None\n",
    "    article.append([title,date,text])\n",
    "\n",
    "# return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(article, desc=\"featurizing articles\"):\n",
    "    foreparts=str(i).split(',')[:2]  # location and date\n",
    "    meat=\"\".join(str(i).split(',')[2:-3])  # article text\n",
    "    cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lobstr API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "lobstr_key = os.environ['LOBSTR_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA['id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 'https://api.lobstr.io/v1/runs/{AA['id'][0]}/stats' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!curl 'https://api.lobstr.io/v1/runs?page=1&page_size=3000' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\\n",
    "    -o 'input/runs.json'\n",
    "    \n",
    "with open(\"input/runs.json\", 'r') as f:\n",
    "    runs = json.load(f)\n",
    "juns=pd.DataFrame(runs['data'])\n",
    "AA=juns[['id','cluster','total_unique_results']]\n",
    "latest_success_run = AA.loc[AA['total_unique_results'].ne(0).idxmax()]\n",
    "\n",
    "!curl 'https://api.lobstr.io/v1/results?cluster=8de6e1bbf33f47b8bce451075b883252&run={latest_success_run['id']}&page=1&page_size=3000' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\\n",
    "    -o 'input/lobstr_results.json'\n",
    "    \n",
    "with open(\"input/lobstr_results.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "jata=pd.DataFrame(data['data'])\n",
    "jata[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input/lobstr_results.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "jata=pd.DataFrame(data['data'])\n",
    "jata[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lobstr via gdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://docs.google.com/spreadsheets/d/178sqEWzqubH0znhx7Z6u9ig2EjCRvl0dUsA7b6hQpmY/export?format=csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(url)\n",
    "df=df[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('input/lobstr_text.txt', sep='\\t', sep='\\t', index_col=0, names=['text'], skiprows=1)\n",
    "url = 'https://docs.google.com/spreadsheets/d/178sqEWzqubH0znhx7Z6u9ig2EjCRvl0dUsA7b6hQpmY/export?format=csv'\n",
    "df = pd.read_csv(url)\n",
    "len(articles) == len(df)\n",
    "# len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_parquet('input/lobstr_text.parquet', index=False)#, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj=pd.read_parquet('input/lobstr_text.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    dataloader = DataLoader(data['text'], batch_size=1, shuffle=True, num_workers=4)\n",
    "    RR = dataloader\n",
    "else:\n",
    "    RR = articles\n",
    "for j,i in tqdm(enumerate(RR), total=len(RR), desc=\"featurizing articles\"):\n",
    "\n",
    "# for i in tqdm(articles, desc=\"featurizing articles\"):\n",
    "    try:\n",
    "        foreparts = str(i).split(',')[:2]  # location and date\n",
    "    except:\n",
    "        foreparts=None\n",
    "    meat=\"\".join(str(j).split(',')[2:-3])  # text\n",
    "    try:\n",
    "        cc=featurize_stories(str(i), top_k = args.f, max_len=512)\n",
    "    except:\n",
    "        'damn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import requests, concurrent\n",
    "from bs4 import BeautifulSoup\n",
    "# def handler(signum, frame):\n",
    "    # raise TimeoutError()/\n",
    "# signal.signal(signal.SIGALRM, handler)\n",
    "\n",
    "\n",
    "def process_url(url):\n",
    "    try:\n",
    "        # signal.alarm(5)\n",
    "        response = requests.get(url,timeout=5)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all(['p'])\n",
    "        text = []\n",
    "        for p in paragraphs:\n",
    "            text.append(p.get_text())\n",
    "        # signal.alarm(0)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing URL {url}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[]\n",
    "# for i in tqdm(df['url'], desc=\"grabbing text from url\"):\n",
    "#     result = process_url(i)\n",
    "#     results.append(result)\n",
    "# df['text'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    result = list(tqdm(executor.map(process_url, df['url']), total=len(df['url'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(result, desc=\"featurizing articles\"):\n",
    "    foreparts = df[['published_at','title']]\n",
    "    try:\n",
    "        cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    except:\n",
    "        cc=None\n",
    "        pass\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rank_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(df['text'], desc=\"featurizing articles\"):\n",
    "    foreparts = df['datepublished_at','title'].tolist()\n",
    "    cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test-google-news-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/WRK/dcolinmorgan/dots'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DOTS.scrape import get_OS_data, get_test_gnews\n",
    "from DOTS.pull import pull_data, process_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import subprocess, json, argparse, os,requests\n",
    "load_dotenv()\n",
    "os_url = os.getenv('OS_TOKEN')\n",
    "lobstr_key = os.getenv('LOBSTR_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grabbing text from url: 100%|██████████| 21/21 [00:02<00:00,  7.13it/s]\n"
     ]
    }
   ],
   "source": [
    "response = get_test_gnews(21)\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "articles = pull_data(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One child has been reported dead, while two children have been reported injured in a shooting after a pupil opened fire at a primary school in Finland. ',\n",
       " 'Finnish police said all those involved in the shooting were 12 years old, including the shooter. ',\n",
       " 'Local police added that one person had been detained and a weapons seized.  ',\n",
       " 'No details about the identities of the children or the condition of the victims were immediately released.',\n",
       " 'The incident occurred at Viertolan Koulu school in the southern Vantaa, a suburb to the capital, Helsinki, at just after 7am on Tuesday morning UK time.',\n",
       " 'The victims were taken to hospital, a police spokesperson told Reuters.',\n",
       " 'The Interior Minister, Mari Rantanen, posted on X: “The day started in a horrifying way. There has been a shooting incident at the Viertola school in Vantaa. I can only imagine the pain and worry that many families are experiencing at the moment. The suspected perpetrator has been caught.”',\n",
       " 'The Prime Minister, Petteri Orpo, said the shooting was deeply shocking.”My thoughts are with the victims, their loved ones and the other students and staff,” he said on X.',\n",
       " 'Local police said the immediate danger at the school was over.',\n",
       " 'Vantaa has a population of around 250,000 people, and is the forth largest city in the country. ',\n",
       " 'Speaking at a press conference reported by Finnish media, Chief of police Ilkka Koskimaki from the Eastern Uusimaa Police Department said the suspect had fully admitted to the attack and that the incident will be investigated as murder and attempted murder.',\n",
       " 'Koskimaki said the handgun used in the attack was owned by a close relative of the suspect.',\n",
       " 'However, the senior policeman didn’t give any indication about the boy’s motive.',\n",
       " 'Finland’s legal system means children under the age of 15 cannot be formally arrested, as they fall beneath the minimum age of criminal liability in Finland.',\n",
       " 'The suspect will now be referred to social services as a result.',\n",
       " 'Previous school shootings in Finland have put a harsh focus on Finland’s gun policy. Finland has one of the highest rates of gun ownership in the world. ',\n",
       " 'There are roughly 1.5 million licensed firearms in Finland, as per government statistics, despite the country’s small population of just 5.6 million. With 430,000 licence-holders in the country, this means roughly 12 per cent of Finns own a firearm',\n",
       " 'Hunting and shooting clubs remain a popular hobby in Finland, where military service, which includes firearms training, remains mandatory for all men over 18. ',\n",
       " 'Firearm use in the country is heavily regulated. Residents looking to get a gun licence in Finland need to supply details about their intended purpose, as well as pass a criminal record check. ',\n",
       " 'This is the third school shooting to have been recorded in Finland. ',\n",
       " 'In 2008, a 22-year-old student shot and killed 10 people with a semi-automatic pistol at a university in South Ostrobothnia in western Finland.',\n",
       " 'An 18-year-old student shot seven of his fellow pupils dead in 2007 in Tuusula in the south of Finland. ',\n",
       " 'Despite high levels of gun ownership, levels of gun violence remain modest by international standards.',\n",
       " 'Finland reported 0.09 gun-related homicides per 100,000 people in 2019, compared to 3.15 in the United States and 20.68 in Mexico. ',\n",
       " 'All rights reserved. © 2021 Associated Newspapers Limited.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from DOTS.feat import featurize_stories\n",
    "import graphistry\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attempted murder', 'finnish police', 'mari rantanen']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_stories(str(articles[12]), top_k = 3, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:20<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "rank_articles=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(articles))):\n",
    "    try:\n",
    "        cc = featurize_stories(str(articles[i]), top_k = 3, max_len=512)\n",
    "        rank_articles.append([cc])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in rank_articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/03/2024 04:06:07 PM - Removing `None` from input X_symbolic DataFrame04/03/2024 04:06:07 PM - process_nodes_dataframes[dirty_cat]04/03/2024 04:06:07 PM - ----------------------------------------04/03/2024 04:06:07 PM - \n",
      "-- Setting Encoder Parts from Fit ::04/03/2024 04:06:07 PM - Feature Columns In: RangeIndex(start=0, stop=10, step=1)04/03/2024 04:06:07 PM - Target Columns In: Index([], dtype='int64')04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ data_encoder ]]:  Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ label_encoder ]]:  Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ scaling_pipeline ]]:  FunctionTransformer(func=functools.partial(<function passthrough_df_cols at 0x34d108180>, columns=RangeIndex(start=0, stop=0, step=1)))\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ scaling_pipeline_target ]]:  FunctionTransformer(func=functools.partial(<function passthrough_df_cols at 0x34d108180>, columns=Index([], dtype='int64')))\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ text_model ]]:  None\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ text_cols ]]:  None\n",
      "04/03/2024 04:06:07 PM - Removing `_n` from input X_symbolic DataFrame04/03/2024 04:06:07 PM - process_nodes_dataframes[dirty_cat]04/03/2024 04:06:07 PM - ----------------------------------------04/03/2024 04:06:07 PM - \n",
      "-- Setting Encoder Parts from Fit ::04/03/2024 04:06:07 PM - Feature Columns In: Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='object')04/03/2024 04:06:07 PM - Target Columns In: Index([], dtype='object')04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ data_encoder ]]:  Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ label_encoder ]]:  Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ scaling_pipeline ]]:  FunctionTransformer(func=functools.partial(<function passthrough_df_cols at 0x34d108180>, columns=Index([], dtype='object')))\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ scaling_pipeline_target ]]:  FunctionTransformer(func=functools.partial(<function passthrough_df_cols at 0x34d108180>, columns=Index([], dtype='object')))\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ text_model ]]:  None\n",
      "04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - [[ text_cols ]]:  None\n",
      "04/03/2024 04:06:07 PM - -Reusing Existing Node Featurization04/03/2024 04:06:07 PM - * Ignoring target column of shape (14, 0) in UMAP fit, as it is not one dimensional04/03/2024 04:06:07 PM - ------------------------------------------------------------------------------------------04/03/2024 04:06:07 PM - Starting UMAP-ing data of shape (14, 0)"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m g2 \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mfeaturize()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# g2._clustersummary()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m g3 \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m g3\u001b[38;5;241m.\u001b[39mdbscan()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/graphistry/umap_utils.py:623\u001b[0m, in \u001b[0;36mUMAPMixin.umap\u001b[0;34m(self, X, y, kind, scale, n_neighbors, min_dist, spread, local_connectivity, repulsion_strength, negative_sample_rate, n_components, metric, suffix, play, encode_position, encode_weight, dbscan, engine, feature_engine, inplace, memoize, verbose, **featurize_kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# add the safe coercion here \u001b[39;00m\n\u001b[1;32m    621\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m make_safe_gpu_dataframes(X_, y_, res\u001b[38;5;241m.\u001b[39mengine)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_umap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemoize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeaturize_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mumap_kwargs\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m res\u001b[38;5;241m.\u001b[39m_weighted_adjacency_nodes \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39m_weighted_adjacency\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39m_xy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/graphistry/umap_utils.py:415\u001b[0m, in \u001b[0;36mUMAPMixin._process_umap\u001b[0;34m(self, res, X_, y_, kind, memoize, featurize_kwargs, verbose, **umap_kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m** Fitting UMAP\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    413\u001b[0m res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mumap_lazy_init(res, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mumap_kwargs_pure)\n\u001b[0;32m--> 415\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_umap_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m res\u001b[38;5;241m.\u001b[39m_xy \u001b[38;5;241m=\u001b[39m emb\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/graphistry/umap_utils.py:305\u001b[0m, in \u001b[0;36mUMAPMixin._umap_fit_transform\u001b[0;34m(self, X, y, verbose)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_umap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUMAP is not initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_umap\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    307\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bundle_embedding(emb, index\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/graphistry/umap_utils.py:288\u001b[0m, in \u001b[0;36mUMAPMixin.umap_fit\u001b[0;34m(self, X, y, verbose)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_umap\u001b[38;5;241m.\u001b[39mgraph_ \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mkneighbors_graph(cc\u001b[38;5;241m.\u001b[39membedding_)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_umap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weighted_adjacency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_umap\u001b[38;5;241m.\u001b[39mgraph_\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# if changing, also update fresh_res\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/umap/umap_.py:2354\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space.\u001b[39;00m\n\u001b[1;32m   2330\u001b[0m \n\u001b[1;32m   2331\u001b[0m \u001b[38;5;124;03m    Optionally use y for supervised dimension reduction.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2351\u001b[0m \u001b[38;5;124;03m                                 Values cannot be infinite.\u001b[39;00m\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2354\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_data \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m   2357\u001b[0m     \u001b[38;5;66;03m# Handle all the optional arguments, setting default\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/sklearn/utils/validation.py:795\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    791\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    792\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[1;32m    793\u001b[0m )\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m--> 795\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[1;32m    798\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.DataFrame(flattened_list)  # each ranked feature is a row\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# g = graphistry.edges(data).materialize_nodes()\n",
    "g = graphistry.nodes(data)\n",
    "g2 = g.featurize()\n",
    "# g2._clustersummary()\n",
    "\n",
    "g3 = g.umap()\n",
    "g3.dbscan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.encode_point_color('_dbscan',palette=[\"hotpink\", \"dodgerblue\"],as_continuous=True).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
