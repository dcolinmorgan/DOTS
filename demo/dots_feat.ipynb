{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run model on sample of OpenSearch tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "def is_installed(package):\n",
    "    try:\n",
    "        pkg_resources.get_distribution(package)\n",
    "        return True\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False\n",
    "\n",
    "print(is_installed('cudf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "# import concurrent.futures\n",
    "# from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch, spacy,nltk,subprocess, json, requests,string,csv\n",
    "\n",
    "model_name = \"distilroberta-base\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "n_gram_range = (1, 2)\n",
    "stop_words = \"english\"\n",
    "embeddings=[]\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "import nltk, string, numpy as np\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npr_news(p):\n",
    "    # Send a GET request to the NPR API\n",
    "    r = requests.get(\"http://api.npr.org/query?apiKey=npr_key\", params=p)\n",
    "\n",
    "    # Parse the XML response to get the story URLs\n",
    "    root = ET.fromstring(r.content)\n",
    "    story_urls = [story.find('link').text for story in root.iter('story')]\n",
    "\n",
    "    # For each story URL, send a GET request to get the HTML content\n",
    "    full_stories = []\n",
    "    for url in story_urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content of the story. This will depend on the structure of the webpage.\n",
    "        # Here, we're assuming that the main content is in a <p> tag. You might need to adjust this depending on the webpage structure.\n",
    "        story = soup.find_all('p')\n",
    "\n",
    "        # Extract the text from the story\n",
    "        full_story = ' '.join(p.text for p in story)\n",
    "        full_stories.append(full_story)\n",
    "    return full_stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_len):\n",
    "    # Tokenize the text into tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Calculate the number of chunks and the size of the final chunk\n",
    "    num_chunks = len(tokens) // max_len\n",
    "    final_chunk_size = len(tokens) % max_len\n",
    "\n",
    "    # If the final chunk is too small, distribute its tokens among the other chunks\n",
    "    if final_chunk_size < max_len / 2:\n",
    "        num_chunks += 1\n",
    "        chunk_sizes = [len(tokens) // num_chunks + (1 if i < len(tokens) % num_chunks else 0) for i in range(num_chunks)]\n",
    "        chunks = [tokens[sum(chunk_sizes[:i]):sum(chunk_sizes[:i+1])] for i in range(num_chunks)]\n",
    "    else:\n",
    "        chunks = [tokens[i:i + max_len] for i in range(0, len(tokens), max_len)]\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def featurize_stories(text, max_len, top_k):\n",
    "    # Extract candidate words/phrases\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "    all_candidates = count.get_feature_names_out()\n",
    "    doc = nlp(text)\n",
    "    noun_phrases = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
    "    nouns = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.add(token.text)\n",
    "\n",
    "    all_nouns = nouns.union(noun_phrases)\n",
    "    candidates = list(filter(lambda candidate: candidate in all_nouns, all_candidates))\n",
    "    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=\"pt\")\n",
    "    # candidate_tokens = {k: v.to(device) for k, v in (candidate_tokens).items()}\n",
    "    candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
    "    candidate_embeddings = candidate_embeddings.detach()#.to_numpy()\n",
    "\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    # chunks = [words[i:i + 512] for i in range(0, len(words), 512)]\n",
    "    chunks = chunk_text(text, max_len)  # use this to chunk better and use less padding thus less memory but also less affect from averging\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text_tokens = tokenizer(chunk, padding=True, return_tensors=\"pt\")\n",
    "        text_tokens = {k: v.to(device) for k, v in (text_tokens).items()}\n",
    "        text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
    "        text_embedding = text_embedding.detach()#.to_numpy()\n",
    "        embeddings.append(text_embedding)\n",
    "    max_emb_shape = max(embedding.shape[0] for embedding in embeddings)\n",
    "    padded_embeddings = [np.pad(embedding.cpu(), ((0, max_emb_shape - embedding.shape[0]), (0, 0))) for embedding in embeddings]\n",
    "    avg_embedding = np.min(padded_embeddings, axis=0)\n",
    "    distances = cosine_similarity(avg_embedding, candidate_embeddings.cpu())\n",
    "    torch.cuda.empty_cache()\n",
    "    return [candidates[index] for index in distances.argsort()[0][::-1][-top_k:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# data=pd.read_csv('/content/drive/MyDrive/consult/Louie_disaster_tweets.csv',header=None)\n",
    "data=pd.read_csv('florida-hurricane-tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('8.5k_florida_tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_articles=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(df),len(data['text']))):\n",
    "    try:\n",
    "        cc=featurize_stories(data['text'][i], max_len=512, top_k=4)\n",
    "        # print(cc)\n",
    "        rank_articles.append(cc)\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in rank_articles for item in sublist]\n",
    "from collections import Counter\n",
    "counter = Counter(flattened_list)\n",
    "df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n",
    "\n",
    "df = df.sort_values(by='Count',ascending=False)\n",
    "df.to_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "print(len(df))\n",
    "# df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [sublist for sublist in rank_articles if any('fire'.lower() in s.lower() for s in sublist)]\n",
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('florida-hurricane-tweet_features.txt',sep='\\t')\n",
    "df2=pd.read_csv('florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "df=pd.concat([df,df2])\n",
    "print(df['Unnamed: 0'])\n",
    "df = df.groupby('Unnamed: 0').sum().sort_values(by='Count',ascending=False)\n",
    "df=df[df['Count']>int(np.round(len(df)*.001))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# nouns = ['apple', 'John', 'London', 'dog', 'Mary', 'Paris', 'banana']\n",
    "nouns= df.reset_index()['Unnamed: 0'].to_list()\n",
    "doc = nlp(' '.join(nouns))\n",
    "\n",
    "proper_nouns = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "print(proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(proper_nouns))\n",
    "proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=['Iceland', '16-01-2024', ['Lava from a volcanic eruption in southwestern Iceland has streamed into a nearby town, engulfing homes and forcing the evacuation of local residents.\\n', 'The Icelandic Meteorological Office said the eruption began just before 8:00 a.m. local time Sunday about a half mile from the town of Grindavík after a series of intense earthquakes.\\n', 'A second fissure opened after noon Sunday and sent lava flows into the town, officials said.\\n', 'Iceland\\'s president, Guðni Th. Jóhannesson, said in an address to the nation on Sunday that a \"daunting period of upheaval\" had begun for those in the Reykjanes peninsula.\\n', '\"We continue to hope for as good an outcome as possible, in the face of these tremendous forces of nature,\" he said.\\n', 'This is the second time in a month that a volcano has erupted just outside Grindavík, a coastal town about 25 miles from the Icelandic capital of Reykjavík.\\n', 'An eruption on December 18 sent lava spewing into the air, but residents had already been told to leave due to heightened seismic activity.\\n', \"Iceland's government said Sunday that Grindavík had been evacuated early that morning and that the eruption isn't expected to reach other populated areas.\\n\", 'So far no flights have been disrupted, and officials are monitoring threats to infrastructure. At least three homes have either burned down or been overtaken by lava, according to the Icelandic broadcaster RUV.\\n', \"The government also said the eruption doesn't present a threat to life.\\n\", 'But in his speech, Jóhannesson offered his sympathy to the loved ones of Lúðvík Pétursson, a man who went missing in a work accident in Grindavík last week.\\n', \"According to Sky News, the 50-year-old Pétursson was filling crevasses formed by volcanic activity and earthquakes when he fell in a crack that had opened after last month's eruption.\\n\", 'Iceland is a hotspot for seismic activity, with 32 active volcanoes. A volcano erupts roughly every five years in the country, though eruptions have occurred more frequently recently. \\n', ' Copyright 2024 NPR. To see more, visit https://www.npr.org.  ', '\\n'], None, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameterize functions\n",
    " - parallelize text pulls from urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, signal,datetime,subprocess,json,os\n",
    "from dotenv import load_dotenv\n",
    "parser = argparse.ArgumentParser(description='Process OS data for dynamic features.')\n",
    "parser.add_argument('-n', type=int, default=10, help='Number of data items to get')\n",
    "parser.add_argument('-f', type=int, default=3, help='Number of features per item to get')\n",
    "parser.add_argument('-o', type=str, default='dots_feats.csv', help='Output file name')\n",
    "parser.add_argument('-p', type=int, default=1, help='Parallelize requests')\n",
    "# parser.add_argument('-s', type=datetime, default=20230101, help='start date')\n",
    "# parser.add_argument('-e', type=datetime, default=20231231, help='end date')\n",
    "args, unknown = parser.parse_known_args()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import get_OS_data\n",
    "dd=get_OS_data(10)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os_url=([os.environ['OS_TOKEN']])\n",
    "\n",
    "os_url = os.getenv('OS_TOKEN')\n",
    "n=10\n",
    "bash_command = f\"\"\"\n",
    "curl -X GET \"{os_url}/emergency-management-news/_search?scroll=1m\" -H 'Content-Type: application/json' -d '{{\n",
    "\"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "    \"size\": {n},\n",
    "    \"query\": {{\n",
    "        \"bool\": {{\n",
    "            \"must\": [\n",
    "                {{\"match_all\": {{}}}}\n",
    "            ]\n",
    "        }}\n",
    "    }}\n",
    "}}'\n",
    "\"\"\"\n",
    "process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "output = process.stdout\n",
    "data = json.loads(output)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.loads(\"input/feat_input.json\")\n",
    "with open(\"input/feat_input.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "data['_scroll_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the scroll ID\n",
    "scroll_id = response['_scroll_id']\n",
    "es = OpenSearch([os.environ['OS_TOKEN']])\n",
    "# Keep scrolling until no more results\n",
    "while len(response['hits']['hits']):\n",
    "    # print(response['hits']['hits'])  # process results\n",
    "\n",
    "    response = es.scroll(\n",
    "        scroll_id=scroll_id,\n",
    "        scroll='1m'  # length of time to keep the scroll window open\n",
    "    )\n",
    "\n",
    "    # Update the scroll ID\n",
    "    scroll_id = response['_scroll_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import featurize_stories, process_data, get_OS_data, process_hit\n",
    "data = get_OS_data(10)\n",
    "data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,argparse,datetime\n",
    "sys.argv = ['dots_feat.py', '-n', '10', '-f', '3', '-o', 'dots_feats.csv', '-p', '1']#, '-s', '20230101', '-e', '20231231']\n",
    "parser = argparse.ArgumentParser(description='Process OS data for dynamic features.')\n",
    "parser.add_argument('-n', type=int, default=10, help='Number of data items to get')\n",
    "parser.add_argument('-f', type=int, default=3, help='Number of features per item to get')\n",
    "parser.add_argument('-o', type=str, default='dots_feats.csv', help='Output file name')\n",
    "parser.add_argument('-p', type=int, default=1, help='Parallelize requests')\n",
    "# parser.add_argument('-s', type=datetime.date, default=datetime.datetime.strptime(20230101), help='start date')\n",
    "# parser.add_argument('-e', type=datetime.date, default=datetime.datetime.strptime(20231231), help='end date')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "from dots.dots_feat import featurize_stories, process_data, get_OS_data, process_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"20230101\"\n",
    "e=\"20231231\"\n",
    "\n",
    "s_date = datetime.datetime.strptime(s, \"%Y%m%d\")\n",
    "e_date = datetime.datetime.strptime(e, \"%Y%m%d\")\n",
    "\n",
    "# Convert datetime objects to ISO 8601 format\n",
    "s_iso = s_date.isoformat() + \"Z\"\n",
    "e_iso = e_date.isoformat() + \"Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s_iso,e_iso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OS_data(n=args.n):\n",
    "    bash_command = f\"\"\"\n",
    "    curl -X GET \"{os_url}\" -H 'Content-Type: application/json' -d '{{\n",
    "\"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "        \"size\": {n},\n",
    "        \"query\": {{\n",
    "            \"bool\": {{\n",
    "                \"must\": [\n",
    "                    {{\"match_all\": {{}}}},\n",
    "                ]\n",
    "            }}\n",
    "        }}\n",
    "    }}'\n",
    "    \"\"\"\n",
    "    process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "    output = process.stdout\n",
    "    data = json.loads(output)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "hits = data['hits']['hits']\n",
    "hit = hits[1]\n",
    "hit['_source']\n",
    "\n",
    "from datetime import datetime\n",
    "source = hit['_source']\n",
    "date = datetime.strptime(source['metadata']['GDELT_DATE'], \"%Y%m%d%H%M%S\")\n",
    "loc = source['metadata']['Locations']\n",
    "# ex = None #source['metadata']['Extras']\n",
    "title = source['metadata']['page_title']\n",
    "url = source['metadata']['DocumentIdentifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = source['metadata']['Locations']\n",
    "\n",
    "s = s.replace(\"'\", '\"')  # json requires double quotes for keys and string values\n",
    "list_of_dicts = json.loads(s)\n",
    "\n",
    "\n",
    "# location_full_name = source['metadata']['Locations']['Location FullName']\n",
    "\n",
    "for dict in list_of_dicts:\n",
    "    if 'Location FullName' in dict:\n",
    "        print(dict['Location FullName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for text, soup,date,loc,title in results:\n",
    "            articles.append([loc,loc,loc,date,title,title,text])\n",
    "            # articles.append(text)\n",
    "\n",
    "            writer.writerow([loc,loc,loc,date,title,title,text]) # force location into top feature, also assume title has important info\n",
    "            # writer.writerow(text)\n",
    "            writer.writerow(['\\n'])\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import process_data_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hits(hit):\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get {url}\")\n",
    "        return z\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all(['p','div','a'])\n",
    "    if not paragraphs:\n",
    "        print(f\"No <p> tags in {url}\")\n",
    "        return z\n",
    "\n",
    "    for p in paragraphs:\n",
    "        z.append(p.get_text())\n",
    "    return z\n",
    "\n",
    "with open('output.html', 'w') as file:\n",
    "    file.write(str(soup))\n",
    "    \n",
    "with open('output.csv', 'w') as file:\n",
    "    file.write(str(z))\n",
    "\n",
    "# return articles\n",
    "articles=[]\n",
    "hits = data['hits']['hits']\n",
    "for hit in hits:\n",
    "    z=process_hits(hit)\n",
    "    articles.append(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_hit(hit):\n",
    "    z = []\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    location = source['metadata'][\"Locations\"]\n",
    "    extras = source['metadata'][\"Extras\"]\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for p in soup.find_all('p'):\n",
    "        z.append(p.get_text())\n",
    "    z.append(page_title)\n",
    "    return z, soup\n",
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for z, soup in results:\n",
    "            writer.writerow(z)\n",
    "            writer.writerow(['\\n'])\n",
    "            articles.append(z)\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_OS_data(10)\n",
    "# articles = process_data(data)\n",
    "hits = data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "results=[]\n",
    "e = concurrent.futures.ThreadPoolExecutor()\n",
    "    # try:\n",
    "future = e.submit(process_hit, hits[1])\n",
    "result = future.result(timeout=5)  # Set timeout for 5 seconds\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hits']['hits'][1]['_source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests,os,csv,logging\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(response):\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    output=[]\n",
    "    for iii,hit in enumerate(hits):\n",
    "\n",
    "        source = hit[\"_source\"]\n",
    "        text=[]\n",
    "        try:\n",
    "            date = datetime.strptime(source['metadata']['GDELT_DATE'], \"%Y%m%d%H%M%S\")\n",
    "            date = formatted_date = date.strftime(\"%d-%m-%Y\")\n",
    "            loc = source['metadata']['Locations']\n",
    "            loc = loc.replace(\"'\", '\"')  # json requires double quotes for keys and string values\n",
    "            try:\n",
    "                list_of_dicts = json.loads(loc)\n",
    "                location_full_names = [dict['Location FullName'] for dict in list_of_dicts if 'Location FullName' in dict]\n",
    "                loc = location_full_names[0]\n",
    "            except:\n",
    "                loc = None\n",
    "            org = source['metadata']['Organizations']\n",
    "            per = source['metadata']['Persons']\n",
    "            theme = source['metadata']['Themes'].rsplit('_')[-1]\n",
    "            title = source['metadata']['page_title']\n",
    "            url = source['metadata']['DocumentIdentifier']\n",
    "            output.append([date, loc, title, org, per, theme, url])\n",
    "            # pagination_id=response['_scroll_id']\n",
    "        \n",
    "        except:\n",
    "            # pagination_id=None\n",
    "            pass\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, client = get_massive_OS_data(args.n,args.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response['hits']['hits']\n",
    "# response[\"_scroll_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=[]\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "while len(hits) != 0:\n",
    "    # try:\n",
    "    response = client.scroll(\n",
    "        scroll='5m',\n",
    "        scroll_id=pagination_id\n",
    "            )\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    article = process_response(response)\n",
    "    articles.append(article)\n",
    "    # except:\n",
    "    #     print(\"A ConnectionTimeout error occurred.\")\n",
    "    #     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flattened_list = [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scroll OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()\n",
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scrape import get_OS_data, get_massive_OS_data,get_google_news\n",
    "from pull import process_hit, process_data,pull_data\n",
    "os.getcwd()\n",
    "# os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scroll_data():\n",
    "    response, client = get_massive_OS_data(1)\n",
    "    pagination_id = response[\"_scroll_id\"]\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    articles=[]\n",
    "    articles2=[]\n",
    "    while len(hits) != 0 and len(articles2) < 11000:\n",
    "        response = client.scroll(\n",
    "            scroll='1m',\n",
    "            scroll_id=pagination_id\n",
    "                )\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        # article = process_data(response)\n",
    "        articles.append(hits)\n",
    "        articles2 = [item for sublist in articles for item in sublist]\n",
    "    return [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(location_str):\n",
    "    if location_str:\n",
    "        try:\n",
    "            location_list = json.loads(location_str.replace(\"'\", '\"'))\n",
    "            return [dict['Location FullName'] for dict in location_list if 'Location FullName' in dict]\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all(['p'])\n",
    "    text = []\n",
    "    for p in paragraphs:\n",
    "        text.append(p.get_text())\n",
    "    return text\n",
    "\n",
    "import json\n",
    "import concurrent.futures, requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [list(d['_source']['metadata'].values()) for d in articles]\n",
    "df = pd.DataFrame(data, columns=['date','title', 'person', 'org', 'location', 'theme', 'text', 'url'])\n",
    "df.date=pd.to_datetime(df.date).dt.strftime('%d-%m-%Y')\n",
    "df['locc'] = df['location'].apply(extract_location)\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    df['text'] = list(executor.map(process_url, df['url']))\n",
    "return df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## small gnews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "from tqdm import tqdm\n",
    "from scrape import get_OS_data, get_massive_OS_data,get_google_news\n",
    "from pull import process_hit, process_data,pull_data\n",
    "# from feat import featurize_stories\n",
    "google_news = GNews()\n",
    "\n",
    "# google_news.period = '7d'  # News from last 7 days\n",
    "google_news.max_results = 10000  # number of responses across a keyword\n",
    "google_news.country = 'United States'  # News from a specific country \n",
    "google_news.language = 'english'  # News in a specific language\n",
    "google_news.exclude_websites = ['yahoo.com', 'cnn.com']  # Exclude news from specific website i.e Yahoo.com and CNN.com\n",
    "google_news.start_date = (2024, 1, 1) # Search from 1st Jan 2020\n",
    "google_news.end_date = (2024, 3, 1) # Search until 1st March 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_resp = google_news.get_news('disaster')\n",
    "article=[]\n",
    "\n",
    "for i in tqdm(range(len(json_resp)), desc=\"grabbing directly from GoogleNews\"):\n",
    "    aa=(google_news.get_full_article(json_resp[i]['url']))\n",
    "    try:\n",
    "        date=aa.publish_date.strftime(\"%d-%m-%Y\")\n",
    "    except:\n",
    "        date=None\n",
    "    try:\n",
    "        title=aa.title\n",
    "        text=aa.text\n",
    "    except:\n",
    "        title=None\n",
    "        text=None\n",
    "    article.append([title,date,text])\n",
    "\n",
    "# return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(article, desc=\"featurizing articles\"):\n",
    "    foreparts=str(i).split(',')[:2]  # location and date\n",
    "    meat=\"\".join(str(i).split(',')[2:-3])  # article text\n",
    "    cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lobstr API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "lobstr_key = os.environ['LOBSTR_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA['id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 'https://api.lobstr.io/v1/runs/{AA['id'][0]}/stats' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!curl 'https://api.lobstr.io/v1/runs?page=1&page_size=3000' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\\n",
    "    -o 'input/runs.json'\n",
    "    \n",
    "with open(\"input/runs.json\", 'r') as f:\n",
    "    runs = json.load(f)\n",
    "juns=pd.DataFrame(runs['data'])\n",
    "AA=juns[['id','cluster','total_unique_results']]\n",
    "latest_success_run = AA.loc[AA['total_unique_results'].ne(0).idxmax()]\n",
    "\n",
    "!curl 'https://api.lobstr.io/v1/results?cluster=8de6e1bbf33f47b8bce451075b883252&run={latest_success_run['id']}&page=1&page_size=3000' \\\n",
    "    -H 'Accept: application/json' \\\n",
    "    -H \"Authorization: Token $lobstr_key\" \\\n",
    "    -o 'input/lobstr_results.json'\n",
    "    \n",
    "with open(\"input/lobstr_results.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "jata=pd.DataFrame(data['data'])\n",
    "jata[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input/lobstr_results.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "jata=pd.DataFrame(data['data'])\n",
    "jata[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lobstr via gdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://docs.google.com/spreadsheets/d/178sqEWzqubH0znhx7Z6u9ig2EjCRvl0dUsA7b6hQpmY/export?format=csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(url)\n",
    "df=df[['published_at','url','title','short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('input/lobstr_text.txt', sep='\\t', sep='\\t', index_col=0, names=['text'], skiprows=1)\n",
    "url = 'https://docs.google.com/spreadsheets/d/178sqEWzqubH0znhx7Z6u9ig2EjCRvl0dUsA7b6hQpmY/export?format=csv'\n",
    "df = pd.read_csv(url)\n",
    "len(articles) == len(df)\n",
    "# len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_parquet('input/lobstr_text.parquet', index=False)#, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj=pd.read_parquet('input/lobstr_text.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    dataloader = DataLoader(data['text'], batch_size=1, shuffle=True, num_workers=4)\n",
    "    RR = dataloader\n",
    "else:\n",
    "    RR = articles\n",
    "for j,i in tqdm(enumerate(RR), total=len(RR), desc=\"featurizing articles\"):\n",
    "\n",
    "# for i in tqdm(articles, desc=\"featurizing articles\"):\n",
    "    try:\n",
    "        foreparts = str(i).split(',')[:2]  # location and date\n",
    "    except:\n",
    "        foreparts=None\n",
    "    meat=\"\".join(str(j).split(',')[2:-3])  # text\n",
    "    try:\n",
    "        cc=featurize_stories(str(i), top_k = args.f, max_len=512)\n",
    "    except:\n",
    "        'damn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import requests, concurrent\n",
    "from bs4 import BeautifulSoup\n",
    "# def handler(signum, frame):\n",
    "    # raise TimeoutError()/\n",
    "# signal.signal(signal.SIGALRM, handler)\n",
    "\n",
    "\n",
    "def process_url(url):\n",
    "    try:\n",
    "        # signal.alarm(5)\n",
    "        response = requests.get(url,timeout=5)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all(['p'])\n",
    "        text = []\n",
    "        for p in paragraphs:\n",
    "            text.append(p.get_text())\n",
    "        # signal.alarm(0)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing URL {url}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[]\n",
    "# for i in tqdm(df['url'], desc=\"grabbing text from url\"):\n",
    "#     result = process_url(i)\n",
    "#     results.append(result)\n",
    "# df['text'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    result = list(tqdm(executor.map(process_url, df['url']), total=len(df['url'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(result, desc=\"featurizing articles\"):\n",
    "    foreparts = df[['published_at','title']]\n",
    "    try:\n",
    "        cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    except:\n",
    "        cc=None\n",
    "        pass\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rank_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farticles = [item for sublist in article for item in sublist]\n",
    "rank_articles=[]\n",
    "for i in tqdm(df['text'], desc=\"featurizing articles\"):\n",
    "    foreparts = df['datepublished_at','title'].tolist()\n",
    "    cc=featurize_stories(str(i), top_k = 3, max_len=512)\n",
    "    rank_articles.append([foreparts,cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits[1]['_index']['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test-google-news-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/WRK/dcolinmorgan/dots'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DOTS.scrape import get_OS_data, get_test_gnews\n",
    "from DOTS.pull import pull_data, process_url\n",
    "from DOTS.ingestion_utils import safe_iter_pull, iter_pull, reduce_newlines, scrape_selenium_headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import subprocess, json, argparse, os,requests\n",
    "load_dotenv()\n",
    "os_url = os.getenv('OS_TOKEN')\n",
    "openai_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "lobstr_key = os.getenv('LOBSTR_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grabbing text from url:  43%|████▎     | 353/821 [00:41<00:51,  9.13it/s]/Users/apple/WRK/dcolinmorgan/dots/DOTS/pull.py:132: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "grabbing text from url: 100%|██████████| 821/821 [01:23<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = get_test_gnews(1000)\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "articles = pull_data(hits)\n",
    "indices = [i for i, x in enumerate(articles) if len(str(x)) < 50]\n",
    "print(len(indices))\n",
    "# for i in indices:\n",
    "#     url = hits[i]['_source']['metadata']['link']\n",
    "#     # print(url)\n",
    "#     try:\n",
    "#         articles[i] = scrape_selenium_headless(url,browser='undetected_chrome')\n",
    "#     except:\n",
    "#         pass\n",
    "# indices2 = [i for i, x in enumerate(articles) if len(str(x)) <50]\n",
    "# print(len(indices2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "indices2 = [i for i, x in enumerate(articles) if len(str(x)) <50]\n",
    "print(len(indices2))\n",
    "articles_not_in_indices2 = [article for i, article in enumerate(articles) if i not in indices2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g.feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()\n",
    "import graphistry\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import subprocess, json, argparse, os,requests\n",
    "load_dotenv()\n",
    "g_pass = os.getenv('g_pass')\n",
    "g_user = os.getenv('g_user')\n",
    "\n",
    "graphistry.register(api=3,protocol=\"https\", server=\"hub.graphistry.com\", username=g_user, password=g_pass) ## key id, secret key\n",
    "from graphistry.features import search_model, topic_model, ngrams_model, ModelDict, default_featurize_parameters, default_umap_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices2 = [i for i, x in enumerate(articles) if len(str(x)) <50]\n",
    "(len(indices2))\n",
    "articles_not_in_indices2 = [article for i, article in enumerate(articles) if i not in indices2]\n",
    "\n",
    "df=pd.Series(articles_not_in_indices2).to_frame()\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "df[object_columns] = df[object_columns].astype(str)\n",
    "df.columns=['text']\n",
    "df['text'] = df['text'].str.replace(\"', '\", ' ')\n",
    "df['text'] = df['text'].str.replace('\", \"', ' ')\n",
    "df['text'] = df['text'].str.replace('\" \"', ' ')\n",
    "df['text'] = df['text'].str.replace(\"' '\", ' ')\n",
    "df['text'] = df['text'].str.replace(\"\\'\", ' ')\n",
    "df['text'] = df['text'].str.replace(\"]\", ' ')\n",
    "df['text'] = df['text'].str.replace(\"[\", ' ')\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "df[object_columns] = df[object_columns].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"TEHRAN, Apr. 02 (MNA) –  Russia s Federal Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Nigerian Army has confirmed a minor expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Raipur, Apr 2 (PTI) In a major anti-insurge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notifications can be managed in browser pref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The police arrested a 19-year-old Rotterdam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>Cloudy this evening with showers after midni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>The latest breaking updates, delivered strai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>WEATHER ALERT Christian Terry, Digital Conte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>LINCOLN, Neb. (KOLN) - A man was cited after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>Copyright 2024 The Associated Press. All Rig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0     \"TEHRAN, Apr. 02 (MNA) –  Russia s Federal Se...\n",
       "1      The Nigerian Army has confirmed a minor expl...\n",
       "2       Raipur, Apr 2 (PTI) In a major anti-insurge...\n",
       "3      Notifications can be managed in browser pref...\n",
       "4      The police arrested a 19-year-old Rotterdam ...\n",
       "..                                                 ...\n",
       "622    Cloudy this evening with showers after midni...\n",
       "623    The latest breaking updates, delivered strai...\n",
       "624    WEATHER ALERT Christian Terry, Digital Conte...\n",
       "625    LINCOLN, Neb. (KOLN) - A man was cited after...\n",
       "626    Copyright 2024 The Associated Press. All Rig...\n",
       "\n",
       "[627 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Ignoring target column of shape (627, 0) in UMAP fit, as it is not one dimensional"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['text: vatandaşlarımızın, nightclub, gündoğdu',\n",
       "       'text: ljlo9b5ncj, h9zep9aikz, louisville',\n",
       "       'text: northeasterly, thursday, snowfall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "g=graphistry.nodes(df)\n",
    "g2 = g.umap(df,**topic_model)\n",
    "g2 = g2.dbscan() #min_dist=1, min_samples=3)\n",
    "g3 = g2.transform_dbscan(df,return_graph=False)\n",
    "df2=pd.DataFrame(g2.get_matrix())\n",
    "\n",
    "max_index_per_row = df2.idxmax(axis=1)\n",
    "top_3_indices = max_index_per_row.value_counts().index[:3]\n",
    "top_3_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <iframe id=\"c27a9279-e639-4bd7-82f0-3fc2bd273e0f\" src=\"https://hub.graphistry.com/graph/graph.html?dataset=86cf0fc5fac94f28bc53d5741b4f35c7&type=arrow&viztoken=cae80eee-8b65-4574-9fbe-a75f9847077d&usertag=7f4412f3-pygraphistry-0.33.7&splashAfter=1712821355&info=true&play=0\"\n",
       "                    allowfullscreen=\"true\" webkitallowfullscreen=\"true\" mozallowfullscreen=\"true\"\n",
       "                    oallowfullscreen=\"true\" msallowfullscreen=\"true\"\n",
       "                    style=\"width:100%; height:500px; border: 1px solid #DDD; overflow: hidden\"\n",
       "                    \n",
       "            >\n",
       "            </iframe>\n",
       "        \n",
       "            <script>\n",
       "                try {\n",
       "                  $(\"#c27a9279-e639-4bd7-82f0-3fc2bd273e0f\").bind('mousewheel', function(e) { e.preventDefault(); });\n",
       "                } catch (e) { console.error('exn catching scroll', e); }\n",
       "            </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DOTS.feat import featurize_stories\n",
    "import graphistry\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tuesday morning', 'morning', 'deterioration']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_stories(str(articles[711]), top_k = 3, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627/627 [14:54<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "rank_articles=[]\n",
    "from tqdm import tqdm\n",
    "indices3 = [i for i in range(len(articles)) if i not in indices2]\n",
    "\n",
    "for i in tqdm(indices3):\n",
    "    try:\n",
    "        cc = featurize_stories(str(articles[i]), top_k = 3, max_len=512)\n",
    "        rank_articles.append([cc])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in rank_articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()\n",
    "import graphistry\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import subprocess, json, argparse, os,requests\n",
    "load_dotenv()\n",
    "g_pass = os.getenv('g_pass')\n",
    "g_user = os.getenv('g_user')\n",
    "\n",
    "graphistry.register(api=3,protocol=\"https\", server=\"hub.graphistry.com\", username=g_user, password=g_pass) ## key id, secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(flattened_list).to_csv('big_test.txt')\n",
    "# data=pd.read_csv('../../../big_test.txt',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2: weather, severe, swathes', '2: thunderstorms, storms, storm',\n",
       "       '1: firefighters, firefighter, daughters'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "data=pd.DataFrame(flattened_list)  # each ranked feature is a row\n",
    "data=data.dropna(how='any')\n",
    "data=data.drop_duplicates()\n",
    "\n",
    "object_columns = data.select_dtypes(include=['object']).columns\n",
    "data[object_columns] = data[object_columns].astype(str)\n",
    "\n",
    "# g = graphistry.edges(data).materialize_nodes()\n",
    "g = graphistry.nodes(data)\n",
    "# g2 = g.featurize()\n",
    "# g2._clustersummary()\n",
    "\n",
    "g22 = g.umap()\n",
    "g22=g22.dbscan()#min_dist=0.1, min_samples=3) #82 groups\n",
    "g33 = g22.transform_dbscan(data,return_graph=False)\n",
    "df22=pd.DataFrame(g22.get_matrix())\n",
    "\n",
    "max_index_per_row = df22.idxmax(axis=1)\n",
    "top_33_indices = max_index_per_row.value_counts().index[:3]\n",
    "top_33_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <iframe id=\"6b733787-f094-46f3-93f2-bb8da6b74576\" src=\"https://hub.graphistry.com/graph/graph.html?dataset=313ed82479c9438b840f1e7360cf8238&type=arrow&viztoken=b87875c6-c94f-4e13-b699-f047b2d9fde0&usertag=7f4412f3-pygraphistry-0.33.7&splashAfter=1712821403&info=true&play=0\"\n",
       "                    allowfullscreen=\"true\" webkitallowfullscreen=\"true\" mozallowfullscreen=\"true\"\n",
       "                    oallowfullscreen=\"true\" msallowfullscreen=\"true\"\n",
       "                    style=\"width:100%; height:500px; border: 1px solid #DDD; overflow: hidden\"\n",
       "                    \n",
       "            >\n",
       "            </iframe>\n",
       "        \n",
       "            <script>\n",
       "                try {\n",
       "                  $(\"#6b733787-f094-46f3-93f2-bb8da6b74576\").bind('mousewheel', function(e) { e.preventDefault(); });\n",
       "                } catch (e) { console.error('exn catching scroll', e); }\n",
       "            </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g22.plot() # .encode_point_color('_dbscan',palette=[\"hotpink\", \"dodgerblue\"],as_continuous=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcoup scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get('https://funcoup.org/downloads/')\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the links on the page\n",
    "links = soup.find_all('a')\n",
    "text = soup.get_text()\n",
    "\n",
    "# Filter the links to only include those that end with .gz\n",
    "gz_links = [link for link in links if link['href'].endswith('.gz')]\n",
    "sizes = re.findall(r'(\\d+\\.\\d+|\\d+)(?=\\s*(MB|KB))', text)\n",
    "\n",
    "# For each .gz link, extract the file name and size\n",
    "species=[]\n",
    "for link, size in zip(gz_links, sizes):\n",
    "    file_name = link.text\n",
    "    file_size = size\n",
    "    species.append([file_name, file_size])\n",
    "species=pd.DataFrame(species, columns=['file_name', 'file_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_kb(size):\n",
    "    value, unit = size\n",
    "    value = float(value)\n",
    "    if unit == 'MB':\n",
    "        value *= 1024  # convert MB to KB\n",
    "    return value\n",
    "\n",
    "# Add a new column 'type' based on the content of 'file_name'\n",
    "species['type'] = species['file_name'].apply(lambda x: 'full' if 'full' in x else ('compact' if 'compact' in x else 'unknown'))\n",
    "\n",
    "# Convert sizes to KB and sort\n",
    "species['file_size_kb'] = species['file_size'].apply(convert_to_kb)\n",
    "species = species.sort_values(by='file_size_kb', ascending=True)\n",
    "\n",
    "# Drop the 'file_size_kb' column if you don't need it\n",
    "species = species.drop(columns=['file_size_kb'])\n",
    "species['name']=species['file_name'].str.split('_').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'file_name' column to a dictionary\n",
    "file_name_dict = species['name'].to_dict()\n",
    "\n",
    "# Make a dictionary where the key and value are the same for each row\n",
    "name_dict = {v: v for k, v in file_name_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a list\n",
    "name_list = list(name_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(name_dict).replace('{', '(').replace('}', ')').replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
