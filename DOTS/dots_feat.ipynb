{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "# import concurrent.futures\n",
    "# from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch, spacy,nltk,subprocess, json, requests,string,csv\n",
    "\n",
    "model_name = \"distilroberta-base\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "n_gram_range = (1, 2)\n",
    "stop_words = \"english\"\n",
    "embeddings=[]\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "import nltk, string, numpy as np\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npr_stories(p):\n",
    "    # Send a GET request to the NPR API\n",
    "    r = requests.get(\"http://api.npr.org/query?apiKey=***\", params=p)\n",
    "\n",
    "    # Parse the XML response to get the story URLs\n",
    "    root = ET.fromstring(r.content)\n",
    "    story_urls = [story.find('link').text for story in root.iter('story')]\n",
    "\n",
    "    # For each story URL, send a GET request to get the HTML content\n",
    "    full_stories = []\n",
    "    for url in story_urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content of the story. This will depend on the structure of the webpage.\n",
    "        # Here, we're assuming that the main content is in a <p> tag. You might need to adjust this depending on the webpage structure.\n",
    "        story = soup.find_all('p')\n",
    "\n",
    "        # Extract the text from the story\n",
    "        full_story = ' '.join(p.text for p in story)\n",
    "        full_stories.append(full_story)\n",
    "    return full_stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_len):\n",
    "    # Tokenize the text into tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Calculate the number of chunks and the size of the final chunk\n",
    "    num_chunks = len(tokens) // max_len\n",
    "    final_chunk_size = len(tokens) % max_len\n",
    "\n",
    "    # If the final chunk is too small, distribute its tokens among the other chunks\n",
    "    if final_chunk_size < max_len / 2:\n",
    "        num_chunks += 1\n",
    "        chunk_sizes = [len(tokens) // num_chunks + (1 if i < len(tokens) % num_chunks else 0) for i in range(num_chunks)]\n",
    "        chunks = [tokens[sum(chunk_sizes[:i]):sum(chunk_sizes[:i+1])] for i in range(num_chunks)]\n",
    "    else:\n",
    "        chunks = [tokens[i:i + max_len] for i in range(0, len(tokens), max_len)]\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def featurize_stories(text, max_len, top_k):\n",
    "    # Extract candidate words/phrases\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "    all_candidates = count.get_feature_names_out()\n",
    "    doc = nlp(text)\n",
    "    noun_phrases = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
    "    nouns = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.add(token.text)\n",
    "\n",
    "    all_nouns = nouns.union(noun_phrases)\n",
    "    candidates = list(filter(lambda candidate: candidate in all_nouns, all_candidates))\n",
    "    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=\"pt\")\n",
    "    # candidate_tokens = {k: v.to(device) for k, v in (candidate_tokens).items()}\n",
    "    candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
    "    candidate_embeddings = candidate_embeddings.detach()#.to_numpy()\n",
    "\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    # chunks = [words[i:i + 512] for i in range(0, len(words), 512)]\n",
    "    chunks = chunk_text(text, max_len)  # use this to chunk better and use less padding thus less memory but also less affect from averging\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text_tokens = tokenizer(chunk, padding=True, return_tensors=\"pt\")\n",
    "        text_tokens = {k: v.to(device) for k, v in (text_tokens).items()}\n",
    "        text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
    "        text_embedding = text_embedding.detach()#.to_numpy()\n",
    "        embeddings.append(text_embedding)\n",
    "    max_emb_shape = max(embedding.shape[0] for embedding in embeddings)\n",
    "    padded_embeddings = [np.pad(embedding.cpu(), ((0, max_emb_shape - embedding.shape[0]), (0, 0))) for embedding in embeddings]\n",
    "    avg_embedding = np.min(padded_embeddings, axis=0)\n",
    "    distances = cosine_similarity(avg_embedding, candidate_embeddings.cpu())\n",
    "    torch.cuda.empty_cache()\n",
    "    return [candidates[index] for index in distances.argsort()[0][::-1][-top_k:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'florida-hurricane-tweet.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# data=pd.read_csv('/content/drive/MyDrive/consult/Louie_disaster_tweets.csv',header=None)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflorida-hurricane-tweet.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/DT/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'florida-hurricane-tweet.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# data=pd.read_csv('/content/drive/MyDrive/consult/Louie_disaster_tweets.csv',header=None)\n",
    "data=pd.read_csv('florida-hurricane-tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('8.5k_florida_tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_articles=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(df),len(data['text']))):\n",
    "    try:\n",
    "        cc=featurize_stories(data['text'][i], max_len=512, top_k=4)\n",
    "        # print(cc)\n",
    "        rank_articles.append(cc)\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in rank_articles for item in sublist]\n",
    "from collections import Counter\n",
    "counter = Counter(flattened_list)\n",
    "df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n",
    "\n",
    "df = df.sort_values(by='Count',ascending=False)\n",
    "df.to_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "print(len(df))\n",
    "# df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [sublist for sublist in rank_articles if any('fire'.lower() in s.lower() for s in sublist)]\n",
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('florida-hurricane-tweet_features.txt',sep='\\t')\n",
    "df2=pd.read_csv('florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "df=pd.concat([df,df2])\n",
    "print(df['Unnamed: 0'])\n",
    "df = df.groupby('Unnamed: 0').sum().sort_values(by='Count',ascending=False)\n",
    "df=df[df['Count']>int(np.round(len(df)*.001))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# nouns = ['apple', 'John', 'London', 'dog', 'Mary', 'Paris', 'banana']\n",
    "nouns= df.reset_index()['Unnamed: 0'].to_list()\n",
    "doc = nlp(' '.join(nouns))\n",
    "\n",
    "proper_nouns = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "print(proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(proper_nouns))\n",
    "proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=['Iceland', '16-01-2024', ['Lava from a volcanic eruption in southwestern Iceland has streamed into a nearby town, engulfing homes and forcing the evacuation of local residents.\\n', 'The Icelandic Meteorological Office said the eruption began just before 8:00 a.m. local time Sunday about a half mile from the town of Grindavík after a series of intense earthquakes.\\n', 'A second fissure opened after noon Sunday and sent lava flows into the town, officials said.\\n', 'Iceland\\'s president, Guðni Th. Jóhannesson, said in an address to the nation on Sunday that a \"daunting period of upheaval\" had begun for those in the Reykjanes peninsula.\\n', '\"We continue to hope for as good an outcome as possible, in the face of these tremendous forces of nature,\" he said.\\n', 'This is the second time in a month that a volcano has erupted just outside Grindavík, a coastal town about 25 miles from the Icelandic capital of Reykjavík.\\n', 'An eruption on December 18 sent lava spewing into the air, but residents had already been told to leave due to heightened seismic activity.\\n', \"Iceland's government said Sunday that Grindavík had been evacuated early that morning and that the eruption isn't expected to reach other populated areas.\\n\", 'So far no flights have been disrupted, and officials are monitoring threats to infrastructure. At least three homes have either burned down or been overtaken by lava, according to the Icelandic broadcaster RUV.\\n', \"The government also said the eruption doesn't present a threat to life.\\n\", 'But in his speech, Jóhannesson offered his sympathy to the loved ones of Lúðvík Pétursson, a man who went missing in a work accident in Grindavík last week.\\n', \"According to Sky News, the 50-year-old Pétursson was filling crevasses formed by volcanic activity and earthquakes when he fell in a crack that had opened after last month's eruption.\\n\", 'Iceland is a hotspot for seismic activity, with 32 active volcanoes. A volcano erupts roughly every five years in the country, though eruptions have occurred more frequently recently. \\n', ' Copyright 2024 NPR. To see more, visit https://www.npr.org.  ', '\\n'], None, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, signal,datetime,subprocess,json,os\n",
    "from dotenv import load_dotenv\n",
    "parser = argparse.ArgumentParser(description='Process OS data for dynamic features.')\n",
    "parser.add_argument('-n', type=int, default=10, help='Number of data items to get')\n",
    "parser.add_argument('-f', type=int, default=3, help='Number of features per item to get')\n",
    "parser.add_argument('-o', type=str, default='dots_feats.csv', help='Output file name')\n",
    "parser.add_argument('-p', type=int, default=1, help='Parallelize requests')\n",
    "# parser.add_argument('-s', type=datetime, default=20230101, help='start date')\n",
    "# parser.add_argument('-e', type=datetime, default=20231231, help='end date')\n",
    "args, unknown = parser.parse_known_args()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import get_data\n",
    "dd=get_data(10)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_url=([os.environ['OS_TOKEN']])\n",
    "n=10\n",
    "bash_command = f\"\"\"\n",
    "curl -X GET \"{os_url}/emergency-management-news/_search?scroll=5m\" -H 'Content-Type: application/json' -d '{{\n",
    "\"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "    \"size\": {n},\n",
    "    \"query\": {{\n",
    "        \"bool\": {{\n",
    "            \"must\": [\n",
    "                {{\"match_all\": {{}}}}\n",
    "            ]\n",
    "        }}\n",
    "    }}\n",
    "}}'\n",
    "\"\"\"\n",
    "process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "output = process.stdout\n",
    "data = json.loads(output)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.loads(\"input/feat_input.json\")\n",
    "with open(\"input/feat_input.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "data['_scroll_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the scroll ID\n",
    "scroll_id = response['_scroll_id']\n",
    "es = OpenSearch([os.environ['OS_TOKEN']])\n",
    "# Keep scrolling until no more results\n",
    "while len(response['hits']['hits']):\n",
    "    # print(response['hits']['hits'])  # process results\n",
    "\n",
    "    response = es.scroll(\n",
    "        scroll_id=scroll_id,\n",
    "        scroll='1m'  # length of time to keep the scroll window open\n",
    "    )\n",
    "\n",
    "    # Update the scroll ID\n",
    "    scroll_id = response['_scroll_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import featurize_stories, process_data, get_data, process_hit\n",
    "data = get_data(10)\n",
    "data['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,argparse,datetime\n",
    "sys.argv = ['dots_feat.py', '-n', '10', '-f', '3', '-o', 'dots_feats.csv', '-p', '1']#, '-s', '20230101', '-e', '20231231']\n",
    "parser = argparse.ArgumentParser(description='Process OS data for dynamic features.')\n",
    "parser.add_argument('-n', type=int, default=10, help='Number of data items to get')\n",
    "parser.add_argument('-f', type=int, default=3, help='Number of features per item to get')\n",
    "parser.add_argument('-o', type=str, default='dots_feats.csv', help='Output file name')\n",
    "parser.add_argument('-p', type=int, default=1, help='Parallelize requests')\n",
    "# parser.add_argument('-s', type=datetime.date, default=datetime.datetime.strptime(20230101), help='start date')\n",
    "# parser.add_argument('-e', type=datetime.date, default=datetime.datetime.strptime(20231231), help='end date')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "from DOTS.dots_feat import featurize_stories, process_data, get_data, process_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"20230101\"\n",
    "e=\"20231231\"\n",
    "\n",
    "s_date = datetime.datetime.strptime(s, \"%Y%m%d\")\n",
    "e_date = datetime.datetime.strptime(e, \"%Y%m%d\")\n",
    "\n",
    "# Convert datetime objects to ISO 8601 format\n",
    "s_iso = s_date.isoformat() + \"Z\"\n",
    "e_iso = e_date.isoformat() + \"Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s_iso,e_iso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n=args.n):\n",
    "    bash_command = f\"\"\"\n",
    "    curl -X GET \"{os_url}\" -H 'Content-Type: application/json' -d '{{\n",
    "\"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "        \"size\": {n},\n",
    "        \"query\": {{\n",
    "            \"bool\": {{\n",
    "                \"must\": [\n",
    "                    {{\"match_all\": {{}}}},\n",
    "                ]\n",
    "            }}\n",
    "        }}\n",
    "    }}'\n",
    "    \"\"\"\n",
    "    process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "    output = process.stdout\n",
    "    data = json.loads(output)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(10)\n",
    "hits = data['hits']['hits']\n",
    "hit = hits[1]\n",
    "hit['_source']\n",
    "\n",
    "from datetime import datetime\n",
    "source = hit['_source']\n",
    "date = datetime.strptime(source['metadata']['GDELT_DATE'], \"%Y%m%d%H%M%S\")\n",
    "loc = source['metadata']['Locations']\n",
    "# ex = None #source['metadata']['Extras']\n",
    "title = source['metadata']['page_title']\n",
    "url = source['metadata']['DocumentIdentifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = source['metadata']['Locations']\n",
    "\n",
    "s = s.replace(\"'\", '\"')  # json requires double quotes for keys and string values\n",
    "list_of_dicts = json.loads(s)\n",
    "\n",
    "\n",
    "# location_full_name = source['metadata']['Locations']['Location FullName']\n",
    "\n",
    "for dict in list_of_dicts:\n",
    "    if 'Location FullName' in dict:\n",
    "        print(dict['Location FullName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for text, soup,date,loc,title in results:\n",
    "            articles.append([loc,loc,loc,date,title,title,text])\n",
    "            # articles.append(text)\n",
    "\n",
    "            writer.writerow([loc,loc,loc,date,title,title,text]) # force location into top feature, also assume title has important info\n",
    "            # writer.writerow(text)\n",
    "            writer.writerow(['\\n'])\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dots_feat import process_data_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hits(hit):\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get {url}\")\n",
    "        return z\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all(['p','div','a'])\n",
    "    if not paragraphs:\n",
    "        print(f\"No <p> tags in {url}\")\n",
    "        return z\n",
    "\n",
    "    for p in paragraphs:\n",
    "        z.append(p.get_text())\n",
    "    return z\n",
    "\n",
    "with open('output.html', 'w') as file:\n",
    "    file.write(str(soup))\n",
    "    \n",
    "with open('output.csv', 'w') as file:\n",
    "    file.write(str(z))\n",
    "\n",
    "# return articles\n",
    "articles=[]\n",
    "hits = data['hits']['hits']\n",
    "for hit in hits:\n",
    "    z=process_hits(hit)\n",
    "    articles.append(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_hit(hit):\n",
    "    z = []\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    location = source['metadata'][\"Locations\"]\n",
    "    extras = source['metadata'][\"Extras\"]\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for p in soup.find_all('p'):\n",
    "        z.append(p.get_text())\n",
    "    z.append(page_title)\n",
    "    return z, soup\n",
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for z, soup in results:\n",
    "            writer.writerow(z)\n",
    "            writer.writerow(['\\n'])\n",
    "            articles.append(z)\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(n=10)\n",
    "# data = get_big_data()\n",
    "# articles = process_data(data)\n",
    "import concurrent.futures\n",
    "articles = process_data_fast(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_articles=[]\n",
    "for i in articles[1:]:\n",
    "    try:\n",
    "        cc=featurize_stories(str(i), max_len=512, top_k=1)\n",
    "        rank_articles.append(cc)\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dots_feats.csv', 'w') as file:\n",
    "    file.write(rank_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import requests,os,csv,logging\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(response):\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    output=[]\n",
    "    for iii,hit in enumerate(hits):\n",
    "\n",
    "        source = hit[\"_source\"]\n",
    "        text=[]\n",
    "        try:\n",
    "            date = datetime.strptime(source['metadata']['GDELT_DATE'], \"%Y%m%d%H%M%S\")\n",
    "            date = formatted_date = date.strftime(\"%d-%m-%Y\")\n",
    "            loc = source['metadata']['Locations']\n",
    "            loc = loc.replace(\"'\", '\"')  # json requires double quotes for keys and string values\n",
    "            try:\n",
    "                list_of_dicts = json.loads(loc)\n",
    "                location_full_names = [dict['Location FullName'] for dict in list_of_dicts if 'Location FullName' in dict]\n",
    "                loc = location_full_names[0]\n",
    "            except:\n",
    "                loc = None\n",
    "            org = source['metadata']['Organizations']\n",
    "            per = source['metadata']['Persons']\n",
    "            theme = source['metadata']['Themes'].rsplit('_')[-1]\n",
    "            title = source['metadata']['page_title']\n",
    "            url = source['metadata']['DocumentIdentifier']\n",
    "            # with open(\"input/report.csv\", \"a\") as f:\n",
    "            #     writer = csv.writer(f)\n",
    "            #     writer.writerow(\n",
    "            #         [\n",
    "            #             date,loc,title,org,per,theme,url,\n",
    "            #         ]\n",
    "            #     )\n",
    "            output.append([date, loc, title, org, per, theme, url])\n",
    "\n",
    "            pagination_id=response['_scroll_id']\n",
    "            # print(iii)\n",
    "        except:\n",
    "            pass\n",
    "    return pagination_id, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_url=([os.environ['OS_TOKEN']])\n",
    "client = OpenSearch(os_url)#,\n",
    "query = {\n",
    "    \"size\": \"1000\",\n",
    "    \"timeout\": \"10s\",\n",
    "    \"slice\": {\n",
    "        \"id\": 0,\n",
    "        \"max\": 10\n",
    "    },\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match_all\": {}},\n",
    "            ]}\n",
    "        },\n",
    "    \"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\",\"metadata.DocumentIdentifier\", \"metadata.Organizations\",\"metadata.Persons\",\"metadata.Themes\",\"metadata.text\", \"metadata.Locations\"],\n",
    "\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    scroll='5m',\n",
    "    body=query,\n",
    ")\n",
    "pagination_id = response[\"_scroll_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'pagination_id' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mscroll(\n\u001b[1;32m      6\u001b[0m     scroll\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5m\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     scroll_id\u001b[38;5;241m=\u001b[39mpagination_id\n\u001b[1;32m      8\u001b[0m         )\n\u001b[1;32m      9\u001b[0m hits \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m pagination_id, article \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m articles\u001b[38;5;241m.\u001b[39mappend(article)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     print(\"A ConnectionTimeout error occurred.\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     pass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mprocess_response\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpagination_id\u001b[49m, output\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'pagination_id' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "articles=[]\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "while len(hits) != 0:\n",
    "    # try:\n",
    "    response = client.scroll(\n",
    "        scroll='5m',\n",
    "        scroll_id=pagination_id\n",
    "            )\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    pagination_id, article = process_response(response)\n",
    "    articles.append(article)\n",
    "    # except:\n",
    "    #     print(\"A ConnectionTimeout error occurred.\")\n",
    "    #     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flattened_list = [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18719"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattened_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
