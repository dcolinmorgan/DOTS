{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# text = \"Forecasters warn of strong wind gusts that could bring down tree limbs and cause power outages. It may also create little to near-zero visibility on the roads. The current storm has wreaked havoc for communities in and around the Sierra Nevada. As of Sunday morning, some 13,000 electric customers in California and Nevada were without power, according to Poweroutage.us. The number of customers experiencing outages in those two states was more than quadruple on Saturday. A section of Interstate 80 remains closed between the California-Nevada state line and the city of Colfax. On Sunday, the California Highway Patrol said there is 'still no estimated time of reopening the freeway.' According to the CHP, 'a mass amount of vehicles' were stuck over Donner Summit on Friday night. Due to the storm, the CHP said it took emergency personnel and tow trucks 'several hours' to reach motorists.\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mElementTree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mET\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "# text = \"Forecasters warn of strong wind gusts that could bring down tree limbs and cause power outages. It may also create little to near-zero visibility on the roads. The current storm has wreaked havoc for communities in and around the Sierra Nevada. As of Sunday morning, some 13,000 electric customers in California and Nevada were without power, according to Poweroutage.us. The number of customers experiencing outages in those two states was more than quadruple on Saturday. A section of Interstate 80 remains closed between the California-Nevada state line and the city of Colfax. On Sunday, the California Highway Patrol said there is 'still no estimated time of reopening the freeway.' According to the CHP, 'a mass amount of vehicles' were stuck over Donner Summit on Friday night. Due to the storm, the CHP said it took emergency personnel and tow trucks 'several hours' to reach motorists.\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "p = {'searchTerm':'\"natural disaster\"','numResults':'10'}\n",
    "def get_npr_stories(p):\n",
    "    # Send a GET request to the NPR API\n",
    "    r = requests.get(\"http://api.npr.org/query?apiKey=MDE5Mzg3Mjc2MDE0MzMyMjM3NjM5ZTI2Ng001\", params=p)\n",
    "\n",
    "    # Parse the XML response to get the story URLs\n",
    "    root = ET.fromstring(r.content)\n",
    "    story_urls = [story.find('link').text for story in root.iter('story')]\n",
    "\n",
    "    # For each story URL, send a GET request to get the HTML content\n",
    "    full_stories = []\n",
    "    for url in story_urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content of the story. This will depend on the structure of the webpage.\n",
    "        # Here, we're assuming that the main content is in a <p> tag. You might need to adjust this depending on the webpage structure.\n",
    "        story = soup.find_all('p')\n",
    "\n",
    "        # Extract the text from the story\n",
    "        full_story = ' '.join(p.text for p in story)\n",
    "        full_stories.append(full_story)\n",
    "    return full_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "n_gram_range = (1, 2)\n",
    "stop_words = \"english\"\n",
    "\n",
    "def featurize_stories(text,top_k):\n",
    "    # Extract candidate words/phrases\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "    all_candidates = count.get_feature_names_out()\n",
    "    doc = nlp(text)\n",
    "    noun_phrases = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
    "    nouns = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.add(token.text)\n",
    "\n",
    "    all_nouns = nouns.union(noun_phrases)\n",
    "    candidates = list(filter(lambda candidate: candidate in all_nouns, all_candidates))\n",
    "\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    model_name = \"distilroberta-base\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=\"pt\")\n",
    "    candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
    "\n",
    "    text_tokens = tokenizer([text], padding=True, return_tensors=\"pt\")\n",
    "    text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
    "    candidate_embeddings = candidate_embeddings.detach().numpy()\n",
    "    text_embedding = text_embedding.detach().numpy()\n",
    "    distances = cosine_similarity(text_embedding, candidate_embeddings)\n",
    "    \n",
    "    return [candidates[index] for index in distances.argsort()[0][-top_k:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "max_len = 512\n",
    "sentences = nltk.sent_tokenize(full_stories[1])\n",
    "clean_sentences = [s.translate(str.maketrans('', '', string.punctuation)).replace('\\n', ' ') for s in full_stories]\n",
    "words = [word for s in sentences for word in nltk.word_tokenize(s.replace('\\n', ' '))]\n",
    "chunks = [' '.join(words[i:i + max_len]) for i in range(0, len(words), max_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
