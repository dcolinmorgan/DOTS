{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "# import concurrent.futures\n",
    "# from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch, spacy,nltk,subprocess, json, requests,string,csv\n",
    "\n",
    "model_name = \"distilroberta-base\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "n_gram_range = (1, 2)\n",
    "stop_words = \"english\"\n",
    "embeddings=[]\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "import nltk, string, numpy as np\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npr_stories(p):\n",
    "    # Send a GET request to the NPR API\n",
    "    r = requests.get(\"http://api.npr.org/query?apiKey=***\", params=p)\n",
    "\n",
    "    # Parse the XML response to get the story URLs\n",
    "    root = ET.fromstring(r.content)\n",
    "    story_urls = [story.find('link').text for story in root.iter('story')]\n",
    "\n",
    "    # For each story URL, send a GET request to get the HTML content\n",
    "    full_stories = []\n",
    "    for url in story_urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content of the story. This will depend on the structure of the webpage.\n",
    "        # Here, we're assuming that the main content is in a <p> tag. You might need to adjust this depending on the webpage structure.\n",
    "        story = soup.find_all('p')\n",
    "\n",
    "        # Extract the text from the story\n",
    "        full_story = ' '.join(p.text for p in story)\n",
    "        full_stories.append(full_story)\n",
    "    return full_stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_len):\n",
    "    # Tokenize the text into tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Calculate the number of chunks and the size of the final chunk\n",
    "    num_chunks = len(tokens) // max_len\n",
    "    final_chunk_size = len(tokens) % max_len\n",
    "\n",
    "    # If the final chunk is too small, distribute its tokens among the other chunks\n",
    "    if final_chunk_size < max_len / 2:\n",
    "        num_chunks += 1\n",
    "        chunk_sizes = [len(tokens) // num_chunks + (1 if i < len(tokens) % num_chunks else 0) for i in range(num_chunks)]\n",
    "        chunks = [tokens[sum(chunk_sizes[:i]):sum(chunk_sizes[:i+1])] for i in range(num_chunks)]\n",
    "    else:\n",
    "        chunks = [tokens[i:i + max_len] for i in range(0, len(tokens), max_len)]\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def featurize_stories(text, max_len, top_k):\n",
    "    # Extract candidate words/phrases\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "    all_candidates = count.get_feature_names_out()\n",
    "    doc = nlp(text)\n",
    "    noun_phrases = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
    "    nouns = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.add(token.text)\n",
    "\n",
    "    all_nouns = nouns.union(noun_phrases)\n",
    "    candidates = list(filter(lambda candidate: candidate in all_nouns, all_candidates))\n",
    "    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=\"pt\")\n",
    "    # candidate_tokens = {k: v.to(device) for k, v in (candidate_tokens).items()}\n",
    "    candidate_embeddings = model(**candidate_tokens)[\"pooler_output\"]\n",
    "    candidate_embeddings = candidate_embeddings.detach()#.to_numpy()\n",
    "\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    # chunks = [words[i:i + 512] for i in range(0, len(words), 512)]\n",
    "    chunks = chunk_text(text, max_len)  # use this to chunk better and use less padding thus less memory but also less affect from averging\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text_tokens = tokenizer(chunk, padding=True, return_tensors=\"pt\")\n",
    "        text_tokens = {k: v.to(device) for k, v in (text_tokens).items()}\n",
    "        text_embedding = model(**text_tokens)[\"pooler_output\"]\n",
    "        text_embedding = text_embedding.detach()#.to_numpy()\n",
    "        embeddings.append(text_embedding)\n",
    "    max_emb_shape = max(embedding.shape[0] for embedding in embeddings)\n",
    "    padded_embeddings = [np.pad(embedding.cpu(), ((0, max_emb_shape - embedding.shape[0]), (0, 0))) for embedding in embeddings]\n",
    "    avg_embedding = np.min(padded_embeddings, axis=0)\n",
    "    distances = cosine_similarity(avg_embedding, candidate_embeddings.cpu())\n",
    "    torch.cuda.empty_cache()\n",
    "    return [candidates[index] for index in distances.argsort()[0][::-1][-top_k:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# data=pd.read_csv('/content/drive/MyDrive/consult/Louie_disaster_tweets.csv',header=None)\n",
    "data=pd.read_csv('florida-hurricane-tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('8.5k_florida_tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_articles=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(df),len(data['text']))):\n",
    "    try:\n",
    "        cc=featurize_stories(data['text'][i], max_len=512, top_k=4)\n",
    "        # print(cc)\n",
    "        rank_articles.append(cc)\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in rank_articles for item in sublist]\n",
    "from collections import Counter\n",
    "counter = Counter(flattened_list)\n",
    "df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n",
    "\n",
    "df = df.sort_values(by='Count',ascending=False)\n",
    "df.to_csv('/content/drive/MyDrive/consult/florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "print(len(df))\n",
    "# df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('florida-hurricane-tweet_features.txt',sep='\\t')\n",
    "df2=pd.read_csv('florida-hurricane-tweet_features2.txt',sep='\\t')\n",
    "df=pd.concat([df,df2])\n",
    "print(df['Unnamed: 0'])\n",
    "df = df.groupby('Unnamed: 0').sum().sort_values(by='Count',ascending=False)\n",
    "df=df[df['Count']>int(np.round(len(df)*.001))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# nouns = ['apple', 'John', 'London', 'dog', 'Mary', 'Paris', 'banana']\n",
    "nouns= df.reset_index()['Unnamed: 0'].to_list()\n",
    "doc = nlp(' '.join(nouns))\n",
    "\n",
    "proper_nouns = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "print(proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(proper_nouns))\n",
    "proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "def get_data(n):\n",
    "    bash_command = f\"\"\"\n",
    "    curl -X GET \"https://louie_armstrong:peach-Jam-42-prt@search-opensearch-dev-domain-7grknmmmm7nikv5vwklw7r4pqq.us-east-1.es.amazonaws.com/emergency-management-news/_search\" -H 'Content-Type: application/json' -d '{{\n",
    "        \"_source\": [\"metadata.GDELT_DATE\", \"metadata.page_title\", \"metadata.DocumentIdentifier\"],\n",
    "        \"size\": {n},\n",
    "        \"query\": {{\n",
    "        \"match_all\": {{}}\n",
    "        }}\n",
    "    }}'\n",
    "    \"\"\"\n",
    "    # print(f\"Running command: {bash_command}\")  # Print the command that will be run\n",
    "\n",
    "    process = subprocess.run(bash_command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    # print(f\"Command output: {process.stdout}\")  # Print the output of the command\n",
    "    # print(f\"Command error: {process.stderr}\")  # Print the error output of the command\n",
    "\n",
    "    output = process.stdout\n",
    "    data = json.loads(output)\n",
    "\n",
    "    # print(f\"Data: {data}\")  # Print the data that will be returned\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(data):\n",
    "    articles=[]\n",
    "    # Extract the specific fields\n",
    "    hits = data['hits']['hits']\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for hit in hits:\n",
    "            z=[]\n",
    "            source = hit['_source']\n",
    "            GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "            # Locations = source['metadata']['Locations']\n",
    "            # Extras = source['metadata']['Extras']\n",
    "            page_title = source['metadata']['page_title']\n",
    "            url = source['metadata']['DocumentIdentifier']\n",
    "            \n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # print(soup.find('content'))\n",
    "            for p in soup.find_all('p'):\n",
    "                z.append(p.get_text())\n",
    "            writer.writerow(z)\n",
    "            writer.writerow(['\\n'])\n",
    "            articles.append(z)\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(soup))\n",
    "        \n",
    "    with open('output.csv', 'w') as file:\n",
    "        file.write(str(z))\n",
    "    \n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_hit(hit):\n",
    "    z = []\n",
    "    source = hit['_source']\n",
    "    GDELT_DATE = source['metadata']['GDELT_DATE']\n",
    "    page_title = source['metadata']['page_title']\n",
    "    url = source['metadata']['DocumentIdentifier']\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for p in soup.find_all('p'):\n",
    "        z.append(p.get_text())\n",
    "    return z, soup\n",
    "\n",
    "def process_data_fast(data):\n",
    "    articles = []\n",
    "    hits = data['hits']['hits']\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_hit, hits))\n",
    "\n",
    "    with open('output.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for z, soup in results:\n",
    "            writer.writerow(z)\n",
    "            writer.writerow(['\\n'])\n",
    "            articles.append(z)\n",
    "\n",
    "    with open('output.html', 'w') as file:\n",
    "        file.write(str(results[-1][1]))  # Write the soup of the last hit\n",
    "\n",
    "    # with open('output.csv', 'w') as file:\n",
    "    #     file.write(str(results[-1][0]))  # Write the z of the last hit\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'root_cause': [{'type': 'parsing_exception',\n",
       "    'reason': 'Unknown key for a START_OBJECT in [size].',\n",
       "    'line': 3,\n",
       "    'col': 17}],\n",
       "  'type': 'parsing_exception',\n",
       "  'reason': 'Unknown key for a START_OBJECT in [size].',\n",
       "  'line': 3,\n",
       "  'col': 17},\n",
       " 'status': 400}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(n=10)\n",
    "# data = get_big_data()\n",
    "# articles = process_data(data)\n",
    "import concurrent.futures\n",
    "articles = process_data_fast(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunk_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m articles[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         cc\u001b[38;5;241m=\u001b[39m\u001b[43mfeaturize_stories\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         rank_articles\u001b[38;5;241m.\u001b[39mappend(cc)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[68], line 28\u001b[0m, in \u001b[0;36mfeaturize_stories\u001b[0;34m(text, max_len, top_k)\u001b[0m\n\u001b[1;32m     24\u001b[0m candidate_embeddings \u001b[38;5;241m=\u001b[39m candidate_embeddings\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;66;03m#.to_numpy()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# words = nltk.word_tokenize(text)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# chunks = [words[i:i + 512] for i in range(0, len(words), 512)]\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_text\u001b[49m(text, max_len)  \u001b[38;5;66;03m# use this to chunk better and use less padding thus less memory but also less affect from averging\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m     31\u001b[0m     text_tokens \u001b[38;5;241m=\u001b[39m tokenizer(chunk, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunk_text' is not defined"
     ]
    }
   ],
   "source": [
    "rank_articles=[]\n",
    "for i in articles[1:]:\n",
    "    try:\n",
    "        cc=featurize_stories(str(i), max_len=512, top_k=1)\n",
    "        rank_articles.append(cc)\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OS_feats.csv', 'w') as file:\n",
    "    file.write(rank_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
